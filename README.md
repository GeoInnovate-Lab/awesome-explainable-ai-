# Recent Publications in Explainable AI and their summaries
A repository for summaries of recent explainable AI/Interpretable ML approaches

### 2015 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](https://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) | KDD | 2015 | N/A | `` | [Link]() |
| [Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model](https://arxiv.org/abs/1511.01644) | arXiv | 2015 | N/A | `` | [Link]() |

### 2016 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf) | KDD | 2016 | N/A | `` | [Link]() |
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) | KDD | 2016 | N/A | `` | [Link]() |
| [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) | arXiv | 2017 | N/A | `Review Paper` | [Link]() |

### 2017 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Transparency: Motivations and Challenges](https://arxiv.org/abs/1708.01870) | arXiv | 2017 | N/A | `Review Paper` | [Link]() |
| [A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) | NeurIPS | 2017 | N/A | `` | [Link]() |
| [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825) | ICML (Workshop) | 2017 | [Github](https://github.com/pair-code/saliency) | `` | [Link]() |
| [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) | ICML | 2017 | N/A | `` | [Link]() |
| [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) | ICML | 2017 | N/A | `` | [Link]() |
| [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ICML | 2017 | N/A | `` | [Link]() |
| [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/final-network-dissection.pdf) | CVPR | 2017 | N/A | `` | [Link]() |

### 2018 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100.pdf) | ACL | 2018 | N/A | `` | [Link]() |
| [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279) | ICML | 2018 | N/A | `` | [Link]() |
| [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399) | HJTL | 2018 | N/A | `` | [Link]() |
| [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) | NeruIPS | 2018 | N/A | `` | [Link]() |
| [Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions](https://arxiv.org/abs/1710.04806) | AAAI | 2018 | N/A | `` | [Link]() |
| [The Mythos of Model Interpretability](https://dl.acm.org/doi/10.1145/3236386.3241340) | arXiv | 2018 | N/A | `Review Paper` | [Link]() |

### 2019 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Human Evaluation of Models Built for Interpretability](https://ojs.aaai.org/index.php/HCOMP/article/view/5280/5132) | AAAI | 2019 | N/A | `Human in the loop` | [Link]() |
| [Data Shapley: Equitable Valuation of Data for Machine Learning](https://arxiv.org/abs/1904.02868) | ICML | 2019 | N/A | `` | [Link]() |
| [Attention is not Explanation](https://arxiv.org/abs/1902.10186) | ACL | 2019 | N/A | `` | [Link]() |
| [Actionable Recourse in Linear Classification](https://arxiv.org/abs/1809.06514) | FAccT | 2019 | N/A | `` | [Link]() |
| [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/abs/1811.10154) | Nature | 2019 | N/A | `` | [Link]() |
| [Explanations can be manipulated and geometry is to blame](https://arxiv.org/abs/1906.07983) | NeurIPS | 2019 | N/A | `` | [Link]() |
| [Learning Optimized Risk Scores](https://arxiv.org/pdf/1610.00168.pdf) | JMLR | 2019 | N/A | `` | [Link]() |
| [Explain Yourself! Leveraging Language Models for Commonsense Reasoning](https://arxiv.org/abs/1906.02361) | ACL | 2019 | N/A | `` | [Link]() |
| [Deep Neural Networks Constrained by Decision Rules](https://ojs.aaai.org/index.php/AAAI/article/view/4095) | AAAI | 2018 | N/A | `` | [Link]() |

### 2020
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786) | CVPR | 2020 | N/A | `` | [Link]() |
| [GANSpace: Discovering Interpretable GAN Controls](https://arxiv.org/abs/2004.02546) | NeurIPS | 2020 | N/A | `` | [Link]() |
| [Explainability for fair machine learning](https://arxiv.org/abs/2010.07389) | arXiv | 2020 | N/A | `` | [Link]() |
| [An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) | Distill | 2020 | N/A | `Tutorial` | [Link]() |
| [Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses](https://arxiv.org/abs/2009.07165) | NeurIPS | 2020 | N/A | `` | [Link]() |
| [Learning Model-Agnostic Counterfactual Explanations for Tabular Data](https://arxiv.org/abs/1910.09398) | WWW | 2020 | N/A | `` | [Link]() |
| [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508) | AIES (AAAI) | 2020 | N/A | `` | [Link]() |
| [Interpreting Interpretability: Understanding Data Scientistsâ€™ Use of Interpretability Tools for Machine Learning](http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf) | CHI | 2020 | N/A | `Review Paper` | [Link]() |
| [Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs](https://dl.acm.org/doi/10.1145/3392878) | arXiv | 2020 | N/A | `Review Paper` | [Link]() |

### 2021
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR (Poster) | 2021 | N/A | `` | [Link]() |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR | 2021 | N/A | `` | [Link]() |
| [Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781) | NeurIPS | 2021 | N/A | `` | [Link]() |
| [Explaining by Removing: A Unified Framework for Model Explanation](https://www.jmlr.org/papers/volume22/20-1316/20-1316.pdf) | JMLR | 2021 | N/A | `` | [Link]() |
| [Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience](https://arxiv.org/abs/2001.09219) | PACMHCI | 2021 | N/A | `` | [Link]() |
| [Towards Robust and Reliable Algorithmic Recourse](https://arxiv.org/abs/2102.13620) | NeurIPS | 2021 | N/A | `` | [Link]() |
| [Algorithmic Recourse: from Counterfactual Explanations to Interventions](https://arxiv.org/abs/2002.06278) | FAccT | 2021 | N/A | `` | [Link]() |
| [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) | CHI | 2021 | N/A | `` | [Link]() |
| [Explainable Reinforcement Learning via Model Transforms](https://arxiv.org/abs/2209.12006) | NeurIPS | 2021 | N/A | `` | [Link]() |

### 2022
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [GlanceNets: Interpretabile, Leak-proof Concept-based Models](https://arxiv.org/abs/2205.15612) | CRL | 2022 | N/A | `` | [Link]() |
| [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) | Transformer Circuit Thread | 2022 | N/A | `Tutorial` | [Link]() |
| [Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329) | EMNLP | 2022 | N/A | `DeepMind` | [Link]() |
| [Interpreting Language Models with Contrastive Explanations](https://arxiv.org/abs/2202.10419) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/pdf/2111.09259.pdf) | PNAS | 2022 | N/A | `DeepMind` `GoogleBrain` | [Link]() |
| [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885) | arXiv | 2022 | [Github](https://github.com/castorini/daam) | `` | [Link]() |
| [Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis](https://arxiv.org/abs/2106.09992) | AISTATS | 2022 | N/A | `` | [Link]() |
| [Use-Case-Grounded Simulations for Explanation Evaluation](https://arxiv.org/abs/2206.02256) | NIPS | 2022 | N/A | `` | [Link]() |
| [The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective](https://arxiv.org/abs/2202.01602) | arXiv | 2022 | N/A | `` | [Link]() |
| [What Makes a Good Explanation?: A Harmonized View of Properties of Explanations](https://arxiv.org/abs/2211.05667) | arXiv | 2022 | N/A | `` | [Link]() |
| [NoiseGrad â€” Enhancing Explanations by Introducing Stochasticity to Model Weights](https://cdn.aaai.org/ojs/20561/20561-13-24574-1-2-20220628.pdf) | AAAI | 2022 | [Github](https://github.com/understandable-machine-intelligence-lab/NoiseGrad) | `` | [Link]() |
| [Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations](https://arxiv.org/abs/2205.07277) | AIES (AAAI) | 2022 | N/A | `` | [Link]() |
| [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models](https://arxiv.org/abs/2202.04053) | arXiv | 2022 | [Github](https://github.com/j-min/DallEval) | `` | [Link]() |
| [Self-explaining deep models with logic rule reasoning](https://arxiv.org/abs/2210.07024) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [What You See is What You Classify: Black Box Attributions](https://arxiv.org/abs/2205.11266) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Concept Activation Regions: A Generalized Framework For Concept-Based Explanations](https://arxiv.org/abs/2209.11222) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](https://arxiv.org/abs/2112.04417) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Scalable Interpretability via Polynomials](https://arxiv.org/abs/2205.14108) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Learning to Scaffold: Optimizing Model Explanations for Teaching](https://arxiv.org/abs/2204.10810) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF](https://arxiv.org/abs/2202.11479) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [WeightedSHAP: analyzing and improving Shapley based feature attribution](https://arxiv.org/abs/2209.13429) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy](https://arxiv.org/abs/2208.00780) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [VICE: Variational Interpretable Concept Embeddings](https://arxiv.org/abs/2205.00756) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Robust Feature-Level Adversaries are Interpretability Tools](https://arxiv.org/abs/2110.03605) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [ProtoX: Explaining a Reinforcement Learning Agent via Prototyping](https://arxiv.org/abs/2211.03162) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model](https://arxiv.org/abs/2210.08151) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability](https://arxiv.org/abs/2108.01335) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Neural Basis Models for Interpretability](https://arxiv.org/abs/2205.14120) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Implications of Model Indeterminacy for Explanations of Automated Decisions](https://proceedings.neurips.cc/paper_files/paper/2022/hash/33201f38001dd381aba2c462051449ba-Abstract-Conference.html) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Explainability Via Causal Self-Talk](https://openreview.net/pdf?id=bk8vkdQfBS) | NeurIPS | 2022 | N/A | `DeepMind` | [Link]() |
| [TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations](https://arxiv.org/abs/2207.04154) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | NeurIPS | 2022 | N/A | `GoogleBrain` | [Link]() |
| [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations](https://arxiv.org/abs/2206.01254) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Interpreting Language Models with Contrastive Explanations](https://aclanthology.org/2022.emnlp-main.14.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://aclanthology.org/2022.emnlp-main.251.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://aclanthology.org/2022.emnlp-main.82.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://aclanthology.org/2022.emnlp-main.310.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://aclanthology.org/2022.emnlp-main.316.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://aclanthology.org/2022.emnlp-main.356.pdf) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://aclanthology.org/2022.emnlp-main.743/) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Optimal Interpretable Clustering Using Oblique Decision Trees](https://dl.acm.org/doi/pdf/10.1145/3534678.3539361) | KDD | 2022 | N/A | `` | [Link]() |
| [ExMeshCNN: An Explainable Convolutional Neural Network Architecture for 3D Shape Analysis](https://dl.acm.org/doi/pdf/10.1145/3534678.3539463) | KDD | 2022 | N/A | `` | [Link]() |
| [Learning Differential Operators for Interpretable Time Series Modeling](https://dl.acm.org/doi/pdf/10.1145/3534678.3539245) | KDD | 2022 | N/A | `` | [Link]() |
| [Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network](https://dl.acm.org/doi/10.1145/3534678.3539276) | KDD | 2022 | N/A | `` | [Link]() |
| [Causal Attention for Interpretable and Generalizable Graph Classification](https://dl.acm.org/doi/10.1145/3534678.3539366) | KDD | 2022 | N/A | `` | [Link]() |
| [Group-wise Reinforcement Feature Generation for Optimal and Explainable Representation Space Reconstruction](https://dl.acm.org/doi/10.1145/3534678.3539278) | KDD | 2022 | N/A | `` | [Link]() |
| [Label-Free Explainability for Unsupervised Models](https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Rethinking Attention-Model Explainability through Faithfulness Violation Test](https://proceedings.mlr.press/v162/liu22i/liu22i.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Hierarchical Shrinkage: Improving the Accuracy and Interpretability of Tree-Based Methods](https://proceedings.mlr.press/v162/agarwal22b/agarwal22b.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [A Functional Information Perspective on Model Interpretation](https://proceedings.mlr.press/v162/gat22a/gat22a.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Inducing Causal Structure for Interpretable Neural Networks](https://proceedings.mlr.press/v162/geiger22a/geiger22a.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder](https://proceedings.mlr.press/v162/kim22g/kim22g.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings](https://proceedings.mlr.press/v162/macdonald22a/macdonald22a.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism](https://proceedings.mlr.press/v162/miao22a/miao22a.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers](https://arxiv.org/abs/2205.08078) | ICML | 2022 | N/A | `` | [Link]() |
| [Robust Models Are More Interpretable Because Attributions Look Normal](https://proceedings.mlr.press/v162/wang22e/wang22e.pdf) | ICML | 2022 | N/A | `` | [Link]() |
| [Latent Diffusion Energy-Based Model for Interpretable Text Modelling](https://proceedings.mlr.press/v162/yu22h/yu22h.pdf) | ICML | 2022 | N/A | `` | [Link]() |


### 2023
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [On the Privacy Risks of Algorithmic Recourse](https://arxiv.org/abs/2211.05427) | AISTATS | 2023 | N/A | `` | [Link]() |
| [Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten](https://arxiv.org/abs/2302.04288) | ICML | 2023 | N/A | `` | [Link]() |
| [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062) | arXiv | 2023 | [Github](https://github.com/deepmind/tracr) | `DeepMind` | [Link]() |
| [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse](https://arxiv.org/abs/2203.06768) | ICLR | 2023 | N/A | `` | [Link]() |
| [Concept-level Debugging of Part-Prototype Networks](https://openreview.net/forum?id=oiwXWPDTyNk) | ICLR | 2023 | N/A | `` | [Link]() |
| [Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning](https://arxiv.org/abs/2203.16464) | ICLR | 2023 | N/A | `` | [Link]() |
| [Re-calibrating Feature Attributions for Model Interpretation](https://openreview.net/pdf?id=WUWJIV2Yxtp) | ICLR | 2023 | N/A | `` | [Link]() |
| [Post-hoc Concept Bottleneck Models](https://arxiv.org/abs/2205.15480) | ICLR | 2023 | N/A | `` | [Link]() |
| [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646) | ICLR | 2023 | N/A | `` | [Link]() |
| [STREET: A Multi-Task Structured Reasoning and Explanation Benchmark](https://arxiv.org/abs/2302.06729) | ICLR | 2023 | N/A | `` | [Link]() |
| [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | [Link]() |
| [EVAL: Explainable Video Anomaly Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_EVAL_Explainable_Video_Anomaly_Localization_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | [Link]() |
| [Overlooked Factors in Concept-based Explanations: Dataset Choice, Concept Learnability, and Human Capability](https://arxiv.org/abs/2207.09615) | CVPR | 2023 | [Github](https://github.com/princetonvisualai/OverlookedFactors) | `` | [Link]() |
| [Spatial-Temporal Concept Based Explanation of 3D ConvNets](https://arxiv.org/abs/2206.05275) | CVPR | 2023 | [Github](https://github.com/yingji425/STCE) | `` | [Link]() |
| [Adversarial Counterfactual Visual Explanations](https://arxiv.org/abs/2303.09962) | CVPR | 2023 | N/A | `` | [Link]() |
| [Bridging the Gap Between Model Explanations in Partially Annotated Multi-Label Classification](https://arxiv.org/abs/2304.01804) | CVPR | 2023 | N/A | `` | [Link]() |
| [Explaining Image Classifiers With Multiscale Directional Image Representation](https://arxiv.org/abs/2211.12857) | CVPR | 2023 | N/A | `` | [Link]() |
| [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://arxiv.org/abs/2211.10154) | CVPR | 2023 | N/A | `` | [Link]() |
| [SketchXAI: A First Look at Explainability for Human Sketches](https://arxiv.org/abs/2304.11744) | CVPR | 2023 | N/A | `` | [Link]() |
| [Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis](https://arxiv.org/abs/2202.07728) | CVPR | 2023 | N/A | `` | [Link]() |
| [Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning](https://arxiv.org/abs/2304.04824) | CVPR | 2023 | N/A | `` | [Link]() |
| [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification](https://arxiv.org/abs/2211.11158) | CVPR | 2023 | N/A | `` | [Link]() |
| [Interpretable Neural-Symbolic Concept Reasoning](https://arxiv.org/pdf/2304.14068.pdf) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| [Identifying Interpretable Subspaces in Image Representations](https://openreview.net/pdf?id=5YUyJYElAc) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat](https://openreview.net/pdf?id=0SgBUsL4W0) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| [Explainability as statistical inference](https://openreview.net/pdf?id=RPzQOi1Cyf) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| [On the Impact of Knowledge Distillation for Model Interpretability](https://openreview.net/pdf?id=XOTFW2BK6i) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| [NA2Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning](https://openreview.net/pdf?id=oUeo2uG1AZ) | ICML (Poster) | 2023 | N/A | `` | [Link]() |
| []() |  |  | N/A | `` | [Link]() |

