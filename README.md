# Recent Publications in Explainable AI
A repository recent explainable AI/Interpretable ML approaches

[If you are would like to contribute to this, feel free and please follow the format: *| [Paper_Title](Paper_Link) | Conference_Name | Year_Published | [Github](Link) | `Keywords` | Any_Summary |* ]

### 2015 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](https://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) | KDD | 2015 | N/A | `` | |
| [Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model](https://arxiv.org/abs/1511.01644) | arXiv | 2015 | N/A | `` | |

### 2016 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf) | KDD | 2016 | N/A | `` | |
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) | KDD | 2016 | N/A | `` | |
| [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) | arXiv | 2017 | N/A | `Review Paper` | |

### 2017 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Transparency: Motivations and Challenges](https://arxiv.org/abs/1708.01870) | arXiv | 2017 | N/A | `Review Paper` | |
| [A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) | NeurIPS | 2017 | N/A | `` | |
| [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825) | ICML (Workshop) | 2017 | [Github](https://github.com/pair-code/saliency) | `` | |
| [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) | ICML | 2017 | N/A | `` | |
| [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) | ICML | 2017 | N/A | `` | |
| [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ICML | 2017 | N/A | `` | |
| [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/final-network-dissection.pdf) | CVPR | 2017 | N/A | `` | |

### 2018 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100.pdf) | ACL | 2018 | N/A | `` | |
| [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279) | ICML | 2018 | N/A | `` | |
| [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399) | HJTL | 2018 | N/A | `` | |
| [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) | NeruIPS | 2018 | N/A | `` | |
| [Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions](https://arxiv.org/abs/1710.04806) | AAAI | 2018 | N/A | `` | |
| [The Mythos of Model Interpretability](https://dl.acm.org/doi/10.1145/3236386.3241340) | arXiv | 2018 | N/A | `Review Paper` | |
| [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) | Nature Machine Intelligence | 2018 | N/A | `` | |

### 2019 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Human Evaluation of Models Built for Interpretability](https://ojs.aaai.org/index.php/HCOMP/article/view/5280/5132) | AAAI | 2019 | N/A | `Human in the loop` | |
| [Data Shapley: Equitable Valuation of Data for Machine Learning](https://arxiv.org/abs/1904.02868) | ICML | 2019 | N/A | `` | |
| [Attention is not Explanation](https://arxiv.org/abs/1902.10186) | ACL | 2019 | N/A | `` | |
| [Actionable Recourse in Linear Classification](https://arxiv.org/abs/1809.06514) | FAccT | 2019 | N/A | `` | |
| [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/abs/1811.10154) | Nature | 2019 | N/A | `` | |
| [Explanations can be manipulated and geometry is to blame](https://arxiv.org/abs/1906.07983) | NeurIPS | 2019 | N/A | `` | |
| [Learning Optimized Risk Scores](https://arxiv.org/pdf/1610.00168.pdf) | JMLR | 2019 | N/A | `` | |
| [Explain Yourself! Leveraging Language Models for Commonsense Reasoning](https://arxiv.org/abs/1906.02361) | ACL | 2019 | N/A | `` | |
| [Deep Neural Networks Constrained by Decision Rules](https://ojs.aaai.org/index.php/AAAI/article/view/4095) | AAAI | 2018 | N/A | `` | |
| [Towards Automatic Concept-based Explanations](https://proceedings.neurips.cc/paper_files/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf) | NeurIPS | 2019 | N/A | `` | |

### 2020
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786) | CVPR | 2020 | N/A | `` | |
| [GANSpace: Discovering Interpretable GAN Controls](https://arxiv.org/abs/2004.02546) | NeurIPS | 2020 | N/A | `` | |
| [Explainability for fair machine learning](https://arxiv.org/abs/2010.07389) | arXiv | 2020 | N/A | `` | |
| [An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) | Distill | 2020 | N/A | `Tutorial` | |
| [Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses](https://arxiv.org/abs/2009.07165) | NeurIPS | 2020 | N/A | `` | |
| [Learning Model-Agnostic Counterfactual Explanations for Tabular Data](https://arxiv.org/abs/1910.09398) | WWW | 2020 | N/A | `` | |
| [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508) | AIES (AAAI) | 2020 | N/A | `` | |
| [Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning](http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf) | CHI | 2020 | N/A | `Review Paper` | |
| [Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs](https://dl.acm.org/doi/10.1145/3392878) | arXiv | 2020 | N/A | `Review Paper` | |
| [Human-Driven FOL Explanations of Deep Learning](https://www.ijcai.org/proceedings/2020/309) | IJCAI       | 2020 | N\A  | 'Logic Explanations' | 
| [A Constraint-Based Approach to Learning and Explanation](https://ojs.aaai.org/index.php/AAAI/article/view/5774) | AAAI | 2020| N\A | 'Mutual Information' |

### 2021
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR (Poster) | 2021 | N/A | `` | |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR | 2021 | N/A | `` | |
| [Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781) | NeurIPS | 2021 | N/A | `` | |
| [Explaining by Removing: A Unified Framework for Model Explanation](https://www.jmlr.org/papers/volume22/20-1316/20-1316.pdf) | JMLR | 2021 | N/A | `` | |
| [Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience](https://arxiv.org/abs/2001.09219) | PACMHCI | 2021 | N/A | `` | |
| [Towards Robust and Reliable Algorithmic Recourse](https://arxiv.org/abs/2102.13620) | NeurIPS | 2021 | N/A | `` | |
| [A Framework to Learn with Interpretation](https://proceedings.neurips.cc/paper/2021/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf) | NeurIPS | 2021 | N/A | `` | |
| [Algorithmic Recourse: from Counterfactual Explanations to Interventions](https://arxiv.org/abs/2002.06278) | FAccT | 2021 | N/A | `` | |
| [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) | CHI | 2021 | N/A | `` | |
| [Explainable Reinforcement Learning via Model Transforms](https://arxiv.org/abs/2209.12006) | NeurIPS | 2021 | N/A | `` | |
| [Aligning Artificial Neural Networks and Ontologies towards Explainable AI](https://ojs.aaai.org/index.php/AAAI/article/view/16626) | AAAI | 2021 | N/A | `` | |

### 2022
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [GlanceNets: Interpretabile, Leak-proof Concept-based Models](https://arxiv.org/abs/2205.15612) | CRL | 2022 | N/A | `` | |
| [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) | Transformer Circuit Thread | 2022 | N/A | `Tutorial` | |
| [Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329) | EMNLP | 2022 | N/A | `DeepMind` | |
| [Interpreting Language Models with Contrastive Explanations](https://arxiv.org/abs/2202.10419) | EMNLP | 2022 | N/A | `` | |
| [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/pdf/2111.09259.pdf) | PNAS | 2022 | N/A | `DeepMind` `GoogleBrain` | |
| [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885) | arXiv | 2022 | [Github](https://github.com/castorini/daam) | `` | |
| [Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis](https://arxiv.org/abs/2106.09992) | AISTATS | 2022 | N/A | `` | |
| [Use-Case-Grounded Simulations for Explanation Evaluation](https://arxiv.org/abs/2206.02256) | NeurIPS | 2022 | N/A | `` | |
| [The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective](https://arxiv.org/abs/2202.01602) | arXiv | 2022 | N/A | `` | |
| [What Makes a Good Explanation?: A Harmonized View of Properties of Explanations](https://arxiv.org/abs/2211.05667) | arXiv | 2022 | N/A | `` | |
| [NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights](https://cdn.aaai.org/ojs/20561/20561-13-24574-1-2-20220628.pdf) | AAAI | 2022 | [Github](https://github.com/understandable-machine-intelligence-lab/NoiseGrad) | `` | |
| [Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations](https://arxiv.org/abs/2205.07277) | AIES (AAAI) | 2022 | N/A | `` | |
| [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models](https://arxiv.org/abs/2202.04053) | arXiv | 2022 | [Github](https://github.com/j-min/DallEval) | `` | |
| [Concept Embedding Models: Beyond the Accuracy-Explainability Trade-Off](https://neurips.cc/Conferences/2022/ScheduleMultitrack?event=52974) | NuerIPS | 2022 | [Github](https://github.com/pietrobarbiero/pytorch_explain) | `CBM`, `CEM` |
| [Self-explaining deep models with logic rule reasoning](https://arxiv.org/abs/2210.07024) | NeurIPS | 2022 | N/A | `` | |
| [What You See is What You Classify: Black Box Attributions](https://arxiv.org/abs/2205.11266) | NeurIPS | 2022 | N/A | `` | |
| [Concept Activation Regions: A Generalized Framework For Concept-Based Explanations](https://arxiv.org/abs/2209.11222) | NeurIPS | 2022 | N/A | `` | |
| [What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](https://arxiv.org/abs/2112.04417) | NeurIPS | 2022 | N/A | `` | |
| [Scalable Interpretability via Polynomials](https://arxiv.org/abs/2205.14108) | NeurIPS | 2022 | N/A | `` | |
| [Learning to Scaffold: Optimizing Model Explanations for Teaching](https://arxiv.org/abs/2204.10810) | NeurIPS | 2022 | N/A | `` | |
| [Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF](https://arxiv.org/abs/2202.11479) | NeurIPS | 2022 | N/A | `` | |
| [WeightedSHAP: analyzing and improving Shapley based feature attribution](https://arxiv.org/abs/2209.13429) | NeurIPS | 2022 | N/A | `` | |
| [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy](https://arxiv.org/abs/2208.00780) | NeurIPS | 2022 | N/A | `` | |
| [VICE: Variational Interpretable Concept Embeddings](https://arxiv.org/abs/2205.00756) | NeurIPS | 2022 | N/A | `` | |
| [Robust Feature-Level Adversaries are Interpretability Tools](https://arxiv.org/abs/2110.03605) | NeurIPS | 2022 | N/A | `` | |
| [ProtoX: Explaining a Reinforcement Learning Agent via Prototyping](https://arxiv.org/abs/2211.03162) | NeurIPS | 2022 | N/A | `` | |
| [ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model](https://arxiv.org/abs/2210.08151) | NeurIPS | 2022 | N/A | `` | |
| [Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability](https://arxiv.org/abs/2108.01335) | NeurIPS | 2022 | N/A | `` | |
| [Neural Basis Models for Interpretability](https://arxiv.org/abs/2205.14120) | NeurIPS | 2022 | N/A | `` | |
| [Implications of Model Indeterminacy for Explanations of Automated Decisions](https://proceedings.neurips.cc/paper_files/paper/2022/hash/33201f38001dd381aba2c462051449ba-Abstract-Conference.html) | NeurIPS | 2022 | N/A | `` | |
| [Explainability Via Causal Self-Talk](https://openreview.net/pdf?id=bk8vkdQfBS) | NeurIPS | 2022 | N/A | `DeepMind` | |
| [TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations](https://arxiv.org/abs/2207.04154) | NeurIPS | 2022 | N/A | `` | |
| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | NeurIPS | 2022 | N/A | `GoogleBrain` | |
| [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | NeurIPS | 2022 | N/A | `` | |
| [Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations](https://arxiv.org/abs/2206.01254) | NeurIPS | 2022 | N/A | `` | |
| [Foundations of Symbolic Languages for Model Interpretability](https://papers.nips.cc/paper_files/paper/2021/hash/60cb558c40e4f18479664069d9642d5a-Abstract.html) | NeurIPS | 2022 | N/A | `` | |
| [The Utility of Explainable AI in Ad Hoc Human-Machine Teaming](https://papers.nips.cc/paper_files/paper/2021/hash/05d74c48b5b30514d8e9bd60320fc8f6-Abstract.html) | NeurIPS | 2022 | N/A | `` | |
| [Interpreting Language Models with Contrastive Explanations](https://aclanthology.org/2022.emnlp-main.14.pdf) | EMNLP | 2022 | N/A | `` | |
| [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://aclanthology.org/2022.emnlp-main.251.pdf) | EMNLP | 2022 | N/A | `` | |
| [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://aclanthology.org/2022.emnlp-main.82.pdf) | EMNLP | 2022 | N/A | `` | |
| [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://aclanthology.org/2022.emnlp-main.310.pdf) | EMNLP | 2022 | N/A | `` | |
| [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://aclanthology.org/2022.emnlp-main.316.pdf) | EMNLP | 2022 | N/A | `` | |
| [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://aclanthology.org/2022.emnlp-main.356.pdf) | EMNLP | 2022 | N/A | `` | |
| [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://aclanthology.org/2022.emnlp-main.743/) | EMNLP | 2022 | N/A | `` | |
| [Optimal Interpretable Clustering Using Oblique Decision Trees](https://dl.acm.org/doi/pdf/10.1145/3534678.3539361) | KDD | 2022 | N/A | `` | |
| [ExMeshCNN: An Explainable Convolutional Neural Network Architecture for 3D Shape Analysis](https://dl.acm.org/doi/pdf/10.1145/3534678.3539463) | KDD | 2022 | N/A | `` | |
| [Learning Differential Operators for Interpretable Time Series Modeling](https://dl.acm.org/doi/pdf/10.1145/3534678.3539245) | KDD | 2022 | N/A | `` | |
| [Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network](https://dl.acm.org/doi/10.1145/3534678.3539276) | KDD | 2022 | N/A | `` | |
| [Causal Attention for Interpretable and Generalizable Graph Classification](https://dl.acm.org/doi/10.1145/3534678.3539366) | KDD | 2022 | N/A | `` | |
| [Group-wise Reinforcement Feature Generation for Optimal and Explainable Representation Space Reconstruction](https://dl.acm.org/doi/10.1145/3534678.3539278) | KDD | 2022 | N/A | `` | |
| [Label-Free Explainability for Unsupervised Models](https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf) | ICML | 2022 | N/A | `` | |
| [Rethinking Attention-Model Explainability through Faithfulness Violation Test](https://proceedings.mlr.press/v162/liu22i/liu22i.pdf) | ICML | 2022 | N/A | `` | |
| [Hierarchical Shrinkage: Improving the Accuracy and Interpretability of Tree-Based Methods](https://proceedings.mlr.press/v162/agarwal22b/agarwal22b.pdf) | ICML | 2022 | N/A | `` | |
| [A Functional Information Perspective on Model Interpretation](https://proceedings.mlr.press/v162/gat22a/gat22a.pdf) | ICML | 2022 | N/A | `` | |
| [Inducing Causal Structure for Interpretable Neural Networks](https://proceedings.mlr.press/v162/geiger22a/geiger22a.pdf) | ICML | 2022 | N/A | `` | |
| [ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder](https://proceedings.mlr.press/v162/kim22g/kim22g.pdf) | ICML | 2022 | N/A | `` | |
| [Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings](https://proceedings.mlr.press/v162/macdonald22a/macdonald22a.pdf) | ICML | 2022 | N/A | `` | |
| [Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism](https://proceedings.mlr.press/v162/miao22a/miao22a.pdf) | ICML | 2022 | N/A | `` | |
| [Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers](https://arxiv.org/abs/2205.08078) | ICML | 2022 | N/A | `` | |
| [Robust Models Are More Interpretable Because Attributions Look Normal](https://proceedings.mlr.press/v162/wang22e/wang22e.pdf) | ICML | 2022 | N/A | `` | |
| [Latent Diffusion Energy-Based Model for Interpretable Text Modelling](https://proceedings.mlr.press/v162/yu22h/yu22h.pdf) | ICML | 2022 | N/A | `` | |
| [Crowd, Expert & AI: A Human-AI Interactive Approach Towards Natural Language Explanation based COVID-19 Misinformation Detection](https://www.ijcai.org/proceedings/2022/0706.pdf) | IJCAI | 2022 | N/A | `` | |
| [AttExplainer: Explain Transformer via Attention by Reinforcement Learning](https://www.ijcai.org/proceedings/2022/0102.pdf) | IJCAI | 2022 | N/A | `` | |
| [Investigating and explaining the frequency bias in classification](https://arxiv.org/abs/2205.03154) | IJCAI | 2022 | N/A | `` | |
| [Counterfactual Interpolation Augmentation (CIA): A Unified Approach to Enhance Fairness and Explainability of DNN](https://www.ijcai.org/proceedings/2022/0103.pdf) | IJCAI | 2022 | N/A | `` | |
| [Axiomatic Foundations of Explainability](https://hal.laas.fr/hal-03702681/document) | IJCAI | 2022 | N/A | `` | |
| [Explaining Soft-Goal Conflicts through Constraint Relaxations](https://www.ijcai.org/proceedings/2022/0634.pdf) | IJCAI | 2022 | N/A | `` | |
| [Robust Interpretable Text Classification against Spurious Correlations Using AND-rules with Negation](https://uia.brage.unit.no/uia-xmlui/handle/11250/3057374) | IJCAI | 2022 | N/A | `` | |
| [Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering](https://arxiv.org/abs/2206.08486) | IJCAI | 2022 | N/A | `` | |
| [Toward Policy Explanations for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2204.12568) | IJCAI | 2022 | N/A | `` | |
| [“My nose is running.” “Are you also coughing?”: Building A Medical Diagnosis Agent with Interpretable Inquiry Logics](https://arxiv.org/abs/2204.13953) | IJCAI | 2022 | N/A | `` | |
| [Model Stealing Defense against Exploiting Information Leak Through the Interpretation of Deep Neural Nets](https://www.ijcai.org/proceedings/2022/0100.pdf) | IJCAI | 2022 | N/A | `` | |
| [Learning by Interpreting](https://www.ijcai.org/proceedings/2022/0609.pdf) | IJCAI | 2022 | N/A | `` | |
| [Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies](https://arxiv.org/abs/2205.01240) | IJCAI | 2022 | N/A | `` | |
| [Explanations for Negative Query Answers under Inconsistency-Tolerant Semantics](https://www.ijcai.org/proceedings/2022/0375.pdf) | IJCAI | 2022 | N/A | `` | |
| [On Preferred Abductive Explanations for Decision Trees and Random Forests](https://hal.science/hal-03764873/) | IJCAI | 2022 | N/A | `` | |
| [Adversarial Explanations for Knowledge Graph Embeddings](https://www.ijcai.org/proceedings/2022/0391.pdf) | IJCAI | 2022 | N/A | `` | |
| [Looking Inside the Black-Box: Logic-based Explanations for Neural Networks](https://proceedings.kr.org/2022/45/) | KR | 2022 | N/A | `` | |
| [Entropy-Based Logic Explanations of Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/20551) | AAAI | 2022 | N/A | `` | |
| [Explainable Neural Rule Learning](https://dl.acm.org/doi/pdf/10.1145/3485447.3512023) | WWW | 2022 | N/A | `` | |
| [Explainable Deep Learning: A Field Guide for the Uninitiated](https://www.jair.org/index.php/jair/article/view/13200) | JAIR | 2022 | N/A | `` | |
| []() |  |  | N/A | `` | |


### 2023
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [On the Privacy Risks of Algorithmic Recourse](https://arxiv.org/abs/2211.05427) | AISTATS | 2023 | N/A | `` | |
| [Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten](https://arxiv.org/abs/2302.04288) | ICML | 2023 | N/A | `` | |
| [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062) | arXiv | 2023 | [Github](https://github.com/deepmind/tracr) | `DeepMind` | |
| [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse](https://arxiv.org/abs/2203.06768) | ICLR | 2023 | N/A | `` | |
| [Concept-level Debugging of Part-Prototype Networks](https://openreview.net/forum?id=oiwXWPDTyNk) | ICLR | 2023 | N/A | `` | |
| [Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning](https://arxiv.org/abs/2203.16464) | ICLR | 2023 | N/A | `` | |
| [Re-calibrating Feature Attributions for Model Interpretation](https://openreview.net/pdf?id=WUWJIV2Yxtp) | ICLR | 2023 | N/A | `` | |
| [Post-hoc Concept Bottleneck Models](https://arxiv.org/abs/2205.15480) | ICLR | 2023 | N/A | `` | |
| [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646) | ICLR | 2023 | N/A | `` | |
| [STREET: A Multi-Task Structured Reasoning and Explanation Benchmark](https://arxiv.org/abs/2302.06729) | ICLR | 2023 | N/A | `` | |
| [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | |
| [EVAL: Explainable Video Anomaly Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_EVAL_Explainable_Video_Anomaly_Localization_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | |
| [Overlooked Factors in Concept-based Explanations: Dataset Choice, Concept Learnability, and Human Capability](https://arxiv.org/abs/2207.09615) | CVPR | 2023 | [Github](https://github.com/princetonvisualai/OverlookedFactors) | `` | |
| [Spatial-Temporal Concept Based Explanation of 3D ConvNets](https://arxiv.org/abs/2206.05275) | CVPR | 2023 | [Github](https://github.com/yingji425/STCE) | `` | |
| [Adversarial Counterfactual Visual Explanations](https://arxiv.org/abs/2303.09962) | CVPR | 2023 | N/A | `` | |
| [Bridging the Gap Between Model Explanations in Partially Annotated Multi-Label Classification](https://arxiv.org/abs/2304.01804) | CVPR | 2023 | N/A | `` | |
| [Explaining Image Classifiers With Multiscale Directional Image Representation](https://arxiv.org/abs/2211.12857) | CVPR | 2023 | N/A | `` | |
| [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://arxiv.org/abs/2211.10154) | CVPR | 2023 | N/A | `` | |
| [SketchXAI: A First Look at Explainability for Human Sketches](https://arxiv.org/abs/2304.11744) | CVPR | 2023 | N/A | `` | |
| [Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis](https://arxiv.org/abs/2202.07728) | CVPR | 2023 | N/A | `` | |
| [Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning](https://arxiv.org/abs/2304.04824) | CVPR | 2023 | N/A | `` | |
| [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification](https://arxiv.org/abs/2211.11158) | CVPR | 2023 | N/A | `` | |
| [Interpretable Neural-Symbolic Concept Reasoning](https://arxiv.org/pdf/2304.14068.pdf) | ICML | 2023 | [Github](https://github.com/pietrobarbiero/pytorch_explain)                                                            |         | |
| [Identifying Interpretable Subspaces in Image Representations](https://openreview.net/pdf?id=5YUyJYElAc) | ICML | 2023 | N/A | `` | |
| [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat](https://openreview.net/pdf?id=0SgBUsL4W0) | ICML | 2023 | N/A | `` | |
| [Explainability as statistical inference](https://openreview.net/pdf?id=RPzQOi1Cyf) | ICML | 2023 | N/A | `` | |
| [On the Impact of Knowledge Distillation for Model Interpretability](https://openreview.net/pdf?id=XOTFW2BK6i) | ICML | 2023 | N/A | `` | |
| [NA2Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning](https://openreview.net/pdf?id=oUeo2uG1AZ) | ICML | 2023 | N/A | `` | |
| [Explaining Reinforcement Learning with Shapley Values](https://openreview.net/pdf?id=R1blujRwj1) | ICML | 2023 | N/A | `` | |
| [Explainable Data-Driven Optimization: From Context to Decision and Back Again](https://openreview.net/pdf?id=4Lk9GHHueJ) | ICML | 2023 | N/A | `` | |
| [Causal Proxy Models for Concept-based Model Explanations](https://openreview.net/pdf?id=1Hh1cIPJ7V) | ICML | 2023 | N/A | `` | |
| [Learning Perturbations to Explain Time Series Predictions](https://openreview.net/pdf?id=WpeZu6WzTB) | ICML | 2023 | N/A | `` | |
| [Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching](https://openreview.net/pdf?id=MocsSAUKlk) | ICML | 2023 | N/A | `` | |
| [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat](https://openreview.net/pdf?id=0SgBUsL4W0) | ICML | 2023 | [Github](https://github.com/batmanlab/ICML-2023-Route-interpret-repeat) | `` | |
| [Representer Point Selection for Explaining Regularized High-dimensional Models](https://openreview.net/pdf?id=GLI2hX4vxx) | ICML | 2023 | N/A | `` | |
| [Towards Explaining Distribution Shifts](https://openreview.net/pdf?id=Tig5ELxc0M) | ICML | 2023 | N/A | `` | |
| [Relevant Walk Search for Explaining Graph Neural Networks](https://openreview.net/pdf?id=BDYIci7bVs) | ICML | 2023 | [Github](https://github.com/xiong-ping/rel_walk_gnnlrp) | `` | |
| [Concept-based Explanations for Out-of-Distribution Detectors](https://openreview.net/pdf?id=a33IYBCFey) | ICML | 2023 | N/A | `` | |
| [GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations](https://openreview.net/pdf?id=KHqQwzx2H2) | ICML | 2023 | N/A | `` | |
| [Robust Explanation for Free or At the Cost of Faithfulness](https://openreview.net/pdf?id=6bfF0RYvMy) | ICML | 2023 | N/A | `` | |
| [Learn to Accumulate Evidence from All Training Samples: Theory and Practice](https://openreview.net/pdf?id=2MaUpKBSju) | ICML | 2023 | N/A | `` | |
| [Towards Trustworthy Explanation: On Causal Rationalization](https://openreview.net/pdf?id=fvTgh4MNUV) | ICML | 2023 | N/A | `` | |
| [Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables](https://openreview.net/pdf?id=BdwGV6fwbK) | ICML | 2023 | N/A | `` | |
| [Probabilistic Concept Bottleneck Models](https://openreview.net/pdf?id=yOxy3T0d6e) | ICML | 2023 | N/A | `` | |
| [What do CNNs Learn in the First Layer and Why? A Linear Systems Perspective](https://openreview.net/pdf?id=RJGad2VFYk) | ICML | 2023 | N/A | `` | |
| [Towards credible visual model interpretation with path attribution](https://openreview.net/pdf?id=cHZBCZmfSo) | ICML | 2023 | N/A | `` | |
| [Trainability, Expressivity and Interpretability in Gated Neural ODEs](https://openreview.net/pdf?id=ZhO8woi9CX) | ICML | 2023 | N/A | `` | |
| [Discover and Cure: Concept-aware Mitigation of Spurious Correlation](https://openreview.net/pdf?id=QDxtrlPmfB) | ICML | 2023 | N/A | `` | |
| [PWSHAP: A Path-Wise Explanation Model for Targeted Variables](https://openreview.net/pdf?id=u8VEJNykA5) | ICML | 2023 | N/A | `` | |
| [A Closer Look at the Intervention Procedure of Concept Bottleneck Models](https://openreview.net/pdf?id=YIWtM3GdZc) | ICML | 2023 | N/A | `` | |
| [Counterfactual Analysis in Dynamic Latent-State Models](https://proceedings.mlr.press/v202/haugh23a/haugh23a.pdf) | ICML | 2023 | N/A | `` | |
| [Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models](https://openreview.net/pdf?id=m5vnLHfNy7) | ICML Workshop | 2023 | N/A | `` | |
| [Rethinking Interpretation: Input-Agnostic Saliency Mapping of Deep Visual Classifiers](https://ojs.aaai.org/index.php/AAAI/article/view/25089) | AAAI | 2023 | N/A | `` | |
| [TopicFM: Robust and Interpretable Topic-Assisted Feature Matching](https://ojs.aaai.org/index.php/AAAI/article/view/25341) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [PEN: Prediction-Explanation Network to Forecast Stock Price Movement with Better Explainability](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [KerPrint: Local-Global Knowledge Graph Enhanced Diagnosis Prediction for Retrospective and Prospective Interpretations](https://ojs.aaai.org/index.php/AAAI/article/view/25667) | AAAI | 2023 | N/A | `` | |
| [Beyond Graph Convolutional Network: An Interpretable Regularizer-Centered Optimization Framework](https://ojs.aaai.org/index.php/AAAI/article/view/25593) | AAAI | 2023 | N/A | `` | |
| [Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling](https://ojs.aaai.org/index.php/AAAI/article/view/25812) | AAAI | 2023 | N/A | `` | |
| [Learning Interpretable Temporal Properties from Positive Examples Only](https://ojs.aaai.org/index.php/AAAI/article/view/25800) | AAAI | 2023 | N/A | `` | |
| [Symbolic Metamodels for Interpreting Black-Boxes Using Primitive Functions](https://ojs.aaai.org/index.php/AAAI/article/view/25816) | AAAI | 2023 | N/A | `` | |
| [Towards More Robust Interpretation via Local Gradient Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/25986) | AAAI | 2023 | N/A | `` | |
| [Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network](https://ojs.aaai.org/index.php/AAAI/article/view/26040) | AAAI | 2023 | N/A | `` | |
| [XClusters: Explainability-First Clustering](https://ojs.aaai.org/index.php/AAAI/article/view/25963) | AAAI | 2023 | N/A | `` | |
| [Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/26267) | AAAI | 2023 | N/A | `` | |
| [Fairness and Explainability: Bridging the Gap towards Fair Model Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/26344) | AAAI | 2023 | N/A | `` | |
| [Explaining Model Confidence Using Counterfactuals](https://ojs.aaai.org/index.php/AAAI/article/view/26399) | AAAI | 2023 | N/A | `` | |
| [SEAT: Stable and Explainable Attention](https://ojs.aaai.org/index.php/AAAI/article/view/26517) | AAAI | 2023 | N/A | `` | |
| [Factual and Informative Review Generation for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26618) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability via Explicit Word Interaction Graph Layer](https://ojs.aaai.org/index.php/AAAI/article/view/26586) | AAAI | 2023 | N/A | `` | |
| [Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing](https://ojs.aaai.org/index.php/AAAI/article/view/26572) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations](https://ojs.aaai.org/index.php/AAAI/article/view/26661) | AAAI | 2023 | N/A | `` | |
| [Targeted Knowledge Infusion To Make Conversational AI Explainable and Safe](https://ojs.aaai.org/index.php/AAAI/article/view/26805) | AAAI | 2023 | N/A | `` | |
| [eForecaster: Unifying Electricity Forecasting with Robust, Flexible, and Explainable Machine Learning Algorithms](https://ojs.aaai.org/index.php/AAAI/article/view/26853) | AAAI | 2023 | N/A | `` | |
| [SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics Manufacturing Using Explainable Artificial Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/26858) | AAAI | 2023 | N/A | `` | |
| [Xaitk-Saliency: An Open Source Explainable AI Toolkit for Saliency](https://ojs.aaai.org/index.php/AAAI/article/view/26871) | AAAI | 2023 | N/A | `` | |
| [Ripple: Concept-Based Interpretation for Raw Time Series Models in Education](https://ojs.aaai.org/index.php/AAAI/article/view/26888) | AAAI | 2023 | N/A | `` | |
| [Semantics, Ontology and Explanation](https://arxiv.org/abs/2304.11124) | arXiv | 2023 | N/A | `Ontological Unpacking` | |
| [Post Hoc Explanations of Language Models Can Improve Language Models](https://arxiv.org/pdf/2305.11426.pdf) | arXiv | 2023 | N/A | `` | |
| [TopicFM: Robust and Interpretable Topic-Assisted Feature Matching](https://ojs.aaai.org/index.php/AAAI/article/view/25341) | AAAI | 2023 | N/A | `` | |
| [Beyond Graph Convolutional Network: An Interpretable Regularizer-Centered Optimization Framework](https://ojs.aaai.org/index.php/AAAI/article/view/25593) | AAAI | 2023 | N/A | `` | |
| [KerPrint: Local-Global Knowledge Graph Enhanced Diagnosis Prediction for Retrospective and Prospective Interpretations](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [PEN: Prediction-Explanation Network to Forecast Stock Price Movement with Better Explainability](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer](https://ojs.aaai.org/index.php/AAAI/article/view/25803) | AAAI | 2023 | N/A | `` | |
| [Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling](https://ojs.aaai.org/index.php/AAAI/article/view/25812) | AAAI | 2023 | N/A | `` | |
| [Learning Interpretable Temporal Properties from Positive Examples Only](https://ojs.aaai.org/index.php/AAAI/article/view/25800) | AAAI | 2023 | N/A | `` | |
| [Unfooling Perturbation-Based Post Hoc Explainers](https://ojs.aaai.org/index.php/AAAI/article/view/25847) | AAAI | 2023 | N/A | `` | |
| [Very Fast, Approximate Counterfactual Explanations for Decision Forests](https://ojs.aaai.org/index.php/AAAI/article/view/25848) | AAAI | 2023 | N/A | `` | |
| [Symbolic Metamodels for Interpreting Black-Boxes Using Primitive Functions](https://ojs.aaai.org/index.php/AAAI/article/view/25816) | AAAI | 2023 | N/A | `` | |
| [Towards More Robust Interpretation via Local Gradient Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/25986) | AAAI | 2023 | N/A | `` | |
| [Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network](https://ojs.aaai.org/index.php/AAAI/article/view/26040) | AAAI | 2023 | N/A | `` | |
| [Local Explanations for Reinforcement Learning](https://ojs.aaai.org/index.php/AAAI/article/view/26081) | AAAI | 2023 | N/A | `` | |
| [XClusters: Explainability-First Clustering](https://ojs.aaai.org/index.php/AAAI/article/view/25963) | AAAI | 2023 | N/A | `` | |
| [Explaining Random Forests Using Bipolar Argumentation and Markov Networks](https://ojs.aaai.org/index.php/AAAI/article/view/26132) | AAAI | 2023 | N/A | `` | |
| [Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/26267) | AAAI | 2023 | N/A | `` | |
| [Fairness and Explainability: Bridging the Gap towards Fair Model Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/26344) | AAAI | 2023 | N/A | `` | |
| [Explaining Model Confidence Using Counterfactuals](https://ojs.aaai.org/index.php/AAAI/article/view/26399) | AAAI | 2023 | N/A | `` | |
| [XRand: Differentially Private Defense against Explanation-Guided Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/26401) | AAAI | 2023 | N/A | `` | |
| [Unsupervised Explanation Generation via Correct Instantiations](https://ojs.aaai.org/index.php/AAAI/article/view/26494) | AAAI | 2023 | N/A | `` | |
| [SEAT: Stable and Explainable Attention](https://ojs.aaai.org/index.php/AAAI/article/view/26517) | AAAI | 2023 | N/A | `` | |
| [Disentangled CVAEs with Contrastive Learning for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26604) | AAAI | 2023 | N/A | `` | |
| [Factual and Informative Review Generation for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26618) | AAAI | 2023 | N/A | `` | |
| [Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing](https://ojs.aaai.org/index.php/AAAI/article/view/26572) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability via Explicit Word Interaction Graph Layer](https://ojs.aaai.org/index.php/AAAI/article/view/26586) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations](https://ojs.aaai.org/index.php/AAAI/article/view/26661) | AAAI | 2023 | N/A | `` | |
| [Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery](https://ojs.aaai.org/index.php/AAAI/article/view/26679) | AAAI | 2023 | N/A | `` | |
| [Monitoring Model Deterioration with Explainable Uncertainty Estimation via Non-parametric Bootstrap](https://ojs.aaai.org/index.php/AAAI/article/view/26755) | AAAI | 2023 | N/A | `` | |
| [Data-Efficient and Interpretable Tabular Anomaly Detection](https://arxiv.org/abs/2203.02034) | KDD | 2023 | N/A | `` | |
| [Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation](https://dl.acm.org/doi/pdf/10.1145/3580305.3599289) | KDD | 2023 | N/A | `` | |
| [Hands-on Tutorial: "Explanations in AI: Methods, Stakeholders and Pitfalls"](https://dl.acm.org/doi/abs/10.1145/3580305.3599181) | KDD | 2023 | N/A | `` | |
| [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599343) | KDD | 2023 | N/A | `` | |
| [Generative AI meets Responsible AI: Practical Challenges and Opportunities](https://dl.acm.org/doi/abs/10.1145/3580305.3599557) | KDD | 2023 | N/A | `` | |
| [Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective](https://dl.acm.org/doi/10.1145/3580305.3599330) | KDD | 2023 | N/A | `` | |
| [MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599435) | KDD | 2023 | N/A | `` | |
| [CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599290) | KDD | 2023 | N/A | `` | |
| [Fire: An Optimization Approach for Fast Interpretable Rule Extraction](https://dl.acm.org/doi/10.1145/3580305.3599353) | KDD | 2023 | N/A | `` | |
| [ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599336) | KDD | 2023 | N/A | `` | |
| [A Causality Inspired Framework for Model Interpretation](https://dl.acm.org/doi/10.1145/3580305.3599240) | KDD | 2023 | N/A | `` | |
| [Path-Specific Counterfactual Fairness for Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3580305.3599462) | KDD | 2023 | N/A | `` | |
| [SURE: Robust, Explainable, and Fair Classification without Sensitive Attributes](https://dl.acm.org/doi/10.1145/3580305.3599514) | KDD | 2023 | N/A | `` | |
| [Learning for Counterfactual Fairness from Observational Data](https://dl.acm.org/doi/10.1145/3580305.3599408) | KDD | 2023 | N/A | `` | |
| [Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks](https://dl.acm.org/doi/abs/10.1145/3580305.3599394) | KDD | 2023 | N/A | `` | |
| [ExplainableFold: Understanding AlphaFold Prediction with Explainable AI](https://dl.acm.org/doi/abs/10.1145/3580305.3599337) | KDD | 2023 | N/A | `` | |
| [FLAMES2Graph: An Interpretable Federated Multivariate Time Series Classification Framework](https://dl.acm.org/doi/abs/10.1145/3580305.3599354) | KDD | 2023 | N/A | `` | |
| [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599343) | KDD | 2023 | N/A | `` | |
| [ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599336) | KDD | 2023 | N/A | `` | |
| [Counterfactual Explanations and Model Multiplicity: a Relational Verification View](https://proceedings.kr.org/2023/78/kr2023-0078-leofante-et-al.pdf) | Proceedings of KR | 2023 | N/A | `` | |
| [Explainable Representations for Relation Prediction in Knowledge Graphs](https://proceedings.kr.org/2023/62/) | Proceedings of KR | 2023 | N/A | `` | |
| [Region-based Saliency Explanations on the Recognition of Facial Genetic Syndromes](https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/64d1a2c33e2b3e04b8de3b24/1691460293298/ID97_Research+Paper_2023.pdf) | PMLR | 2023 | N/A | `` | |
| [FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods](https://arxiv.org/pdf/2308.06248.pdf) | arXiv | 2023 | N/A | `` | |
| [Diffusion-based Visual Counterfactual Explanations - Towards Systematic Quantitative Evaluation](https://arxiv.org/pdf/2308.06100.pdf) | arXiv | 2023 | N/A | `` | |
| [Testing methods of neural systems understanding](https://www.sciencedirect.com/science/article/pii/S1389041723000906) | Cognitive Systems Research | 2023 | N/A | `` | |
| [Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning](https://arxiv.org/pdf/2308.03999.pdf) | arXiv | 2023 | N/A | `` | |
| [An Explainable Federated Learning and Blockchain based Secure Credit Modeling Method](https://www.sciencedirect.com/science/article/abs/pii/S0377221723006677) | EJOR | 2023 | N/A | `` | |
| [i-Align: an interpretable knowledge graph alignment model](https://link.springer.com/article/10.1007/s10618-023-00963-3) | DMKD | 2023 | N/A | `` | |
| [Goodhart’s Law Applies to NLP’s Explanation Benchmarks](https://arxiv.org/pdf/2308.14272.pdf) | arXiv | 2023 | N/A | `` | |
| [DELELSTM: DECOMPOSITION-BASED LINEAR EXPLAINABLE LSTM TO CAPTURE INSTANTANEOUS AND LONG-TERM EFFECTS IN TIME SERIES](https://arxiv.org/pdf/2308.13797.pdf) | arXiv | 2023 | N/A | `` | |
| [BEYOND DISCRIMINATIVE REGIONS: SALIENCY MAPS AS ALTERNATIVES TO CAMS FOR WEAKLY SU- PERVISED SEMANTIC SEGMENTATION](https://arxiv.org/pdf/2308.11052.pdf) | arXiv | 2023 | N/A | `` | |
| [SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks](https://arxiv.org/pdf/2308.11845.pdf) | arXiv | 2023 | N/A | `` | |
| [Sparse Linear Concept Discovery Models](https://arxiv.org/pdf/2308.10782.pdf) | arXiv | 2023 | N/A | `` | |
| [Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)](https://arxiv.org/pdf/2307.14239.pdf) | arXiv | 2023 | N/A | `` | |
| [KGTN: Knowledge Graph Transformer Network for explainable multi-category item recommendation](https://www.sciencedirect.com/science/article/abs/pii/S0950705123006044) | KBS | 2023 | N/A | `` | |
| [SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems](https://arxiv.org/pdf/2307.15786.pdf) | arXiv | 2023 | N/A | `` | |
| [Explainable Multi-Agent Reinforcement Learning for Temporal Queries](https://www.ijcai.org/proceedings/2023/0007.pdf) | IJCAI | 2023 | N/A | `` | |
| [Advancing Post-Hoc Case-Based Explanation with Feature Highlighting](https://www.ijcai.org/proceedings/2023/0048.pdf) | IJCAI | 2023 | N/A | `` | |
| [Explanation-Guided Reward Alignment](https://www.ijcai.org/proceedings/2023/0053.pdf) | IJCAI | 2023 | N/A | `` | |
| [FEAMOE: Fair, Explainable and Adaptive Mixture of Experts](https://www.ijcai.org/proceedings/2023/0055.pdf) | IJCAI | 2023 | N/A | `` | |
| [Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs](https://www.ijcai.org/proceedings/2023/0058.pdf) | IJCAI | 2023 | N/A | `` | |
| [Learning Prototype Classifiers for Long-Tailed Recognition](https://www.ijcai.org/proceedings/2023/0151.pdf) | IJCAI | 2023 | N/A | `` | |
| [On Translations between ML Models for XAI Purposes](https://www.ijcai.org/proceedings/2023/0352.pdf) | IJCAI | 2023 | N/A | `` | |
| [The Parameterized Complexity of Finding Concise Local Explanations](https://www.ijcai.org/proceedings/2023/0369.pdf) | IJCAI | 2023 | N/A | `` | |
| [Neuro-Symbolic Class Expression Learning](https://www.ijcai.org/proceedings/2023/0403.pdf) | IJCAI | 2023 | N/A | `` | |
| [A Logic-based Approach to Contrastive Explainability for Neurosymbolic Visual Question Answering](https://www.ijcai.org/proceedings/2023/0408.pdf) | IJCAI | 2023 | N/A | `` | |
| [Cardinality-Minimal Explanations for Monotonic Neural Networks](https://www.ijcai.org/proceedings/2023/0409.pdf) | IJCAI | 2023 | N/A | `` | |
| [Unveiling Concepts Learned by a World-Class Chess-Playing Agent](https://www.ijcai.org/proceedings/2023/0541.pdf) | IJCAI | 2023 | N/A | `` | |
| [Explainable Text Classification via Attentive and Targeted Mixing Data Augmentation](https://www.ijcai.org/proceedings/2023/0565.pdf) | IJCAI | 2023 | N/A | `` | |
| [On the Complexity of Counterfactual Reasoning](https://www.ijcai.org/proceedings/2023/0630.pdf) | IJCAI | 2023 | N/A | `` | |
| [Interpretable Local Concept-based Explanation with Human Feedback to Predict All-cause Mortality (Extended Abstract)](https://www.ijcai.org/proceedings/2023/0774.pdf) | IJCAI | 2023 | N/A | `` | |
| [Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing](https://arxiv.org/pdf/2309.05679.pdf) | arXiv | 2023 | N/A | `` | |
| [Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse](https://arxiv.org/pdf/2309.04211.pdf) | arXiv | 2023 | N/A | `` | |
| [Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations](https://arxiv.org/pdf/2309.04676.pdf) | CIKM | 2023 | N/A | `` | |
| [A Function Interpretation Benchmark for Evaluating Interpretability Methods](https://arxiv.org/pdf/2309.03886.pdf) | arXiv | 2023 | N/A | `` | |
| [Explaining through Transformer Input Sampling](https://dial.uclouvain.be/pr/boreal/object/boreal%3A277791/datastream/PDF_01/view) | arXiv | 2023 | N/A | `` | |
| []() |  |  | N/A | `` | |
| []() |  |  | N/A | `` | |
| []() |  |  | N/A | `` | |
| []() |  |  | N/A | `` | |
