# Recent Publications in Explainable AI
A repository containing recent explainable AI/Interpretable ML approaches


### 2015 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](https://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) | KDD | 2015 | N/A | `` | |
| [Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model](https://arxiv.org/abs/1511.01644) | arXiv | 2015 | N/A | `` | |

### 2016 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf) | KDD | 2016 | N/A | `` | |
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) | KDD | 2016 | N/A | `` | |
| [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) | arXiv | 2017 | N/A | `Review Paper` | |

### 2017 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Transparency: Motivations and Challenges](https://arxiv.org/abs/1708.01870) | arXiv | 2017 | N/A | `Review Paper` | |
| [A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) | NeurIPS | 2017 | N/A | `` | |
| [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825) | ICML (Workshop) | 2017 | [Github](https://github.com/pair-code/saliency) | `` | |
| [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) | ICML | 2017 | N/A | `` | |
| [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) | ICML | 2017 | N/A | `` | |
| [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ICML | 2017 | N/A | `` | |
| [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/final-network-dissection.pdf) | CVPR | 2017 | N/A | `` | |

### 2018 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100.pdf) | ACL | 2018 | N/A | `` | |
| [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279) | ICML | 2018 | N/A | `` | |
| [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399) | HJTL | 2018 | N/A | `` | |
| [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) | NeruIPS | 2018 | N/A | `` | |
| [Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions](https://arxiv.org/abs/1710.04806) | AAAI | 2018 | N/A | `` | |
| [The Mythos of Model Interpretability](https://dl.acm.org/doi/10.1145/3236386.3241340) | arXiv | 2018 | N/A | `Review Paper` | |
| [Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead](https://www.nature.com/articles/s42256-019-0048-x) | Nature Machine Intelligence | 2018 | N/A | `` | |

### 2019 
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Human Evaluation of Models Built for Interpretability](https://ojs.aaai.org/index.php/HCOMP/article/view/5280/5132) | AAAI | 2019 | N/A | `Human in the loop` | |
| [Data Shapley: Equitable Valuation of Data for Machine Learning](https://arxiv.org/abs/1904.02868) | ICML | 2019 | N/A | `` | |
| [Attention is not Explanation](https://arxiv.org/abs/1902.10186) | ACL | 2019 | N/A | `` | |
| [Actionable Recourse in Linear Classification](https://arxiv.org/abs/1809.06514) | FAccT | 2019 | N/A | `` | |
| [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/abs/1811.10154) | Nature | 2019 | N/A | `` | |
| [Explanations can be manipulated and geometry is to blame](https://arxiv.org/abs/1906.07983) | NeurIPS | 2019 | N/A | `` | |
| [Learning Optimized Risk Scores](https://arxiv.org/pdf/1610.00168.pdf) | JMLR | 2019 | N/A | `` | |
| [Explain Yourself! Leveraging Language Models for Commonsense Reasoning](https://arxiv.org/abs/1906.02361) | ACL | 2019 | N/A | `` | |
| [Deep Neural Networks Constrained by Decision Rules](https://ojs.aaai.org/index.php/AAAI/article/view/4095) | AAAI | 2018 | N/A | `` | |
| [Towards Automatic Concept-based Explanations](https://proceedings.neurips.cc/paper_files/paper/2019/file/77d2afcb31f6493e350fca61764efb9a-Paper.pdf) | NeurIPS | 2019 | [Github](https://github.com/amiratag/ACE) | `` | |

### 2020
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786) | CVPR | 2020 | N/A | `` | |
| [GANSpace: Discovering Interpretable GAN Controls](https://arxiv.org/abs/2004.02546) | NeurIPS | 2020 | N/A | `` | |
| [Explainability for fair machine learning](https://arxiv.org/abs/2010.07389) | arXiv | 2020 | N/A | `` | |
| [An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) | Distill | 2020 | N/A | `Tutorial` | |
| [Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses](https://arxiv.org/abs/2009.07165) | NeurIPS | 2020 | N/A | `` | |
| [Learning Model-Agnostic Counterfactual Explanations for Tabular Data](https://arxiv.org/abs/1910.09398) | WWW | 2020 | N/A | `` | |
| [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508) | AIES (AAAI) | 2020 | N/A | `` | |
| [Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning](http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf) | CHI | 2020 | N/A | `Review Paper` | |
| [Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs](https://dl.acm.org/doi/10.1145/3392878) | arXiv | 2020 | N/A | `Review Paper` | |
| [Human-Driven FOL Explanations of Deep Learning](https://www.ijcai.org/proceedings/2020/309) | IJCAI       | 2020 | N\A  | 'Logic Explanations' | 
| [A Constraint-Based Approach to Learning and Explanation](https://ojs.aaai.org/index.php/AAAI/article/view/5774) | AAAI | 2020| N\A | 'Mutual Information' |

### 2021
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR (Poster) | 2021 | N/A | `` | |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR | 2021 | N/A | `` | |
| [Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781) | NeurIPS | 2021 | N/A | `` | |
| [Explaining by Removing: A Unified Framework for Model Explanation](https://www.jmlr.org/papers/volume22/20-1316/20-1316.pdf) | JMLR | 2021 | N/A | `` | |
| [Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience](https://arxiv.org/abs/2001.09219) | PACMHCI | 2021 | N/A | `` | |
| [Towards Robust and Reliable Algorithmic Recourse](https://arxiv.org/abs/2102.13620) | NeurIPS | 2021 | N/A | `` | |
| [A Framework to Learn with Interpretation](https://proceedings.neurips.cc/paper/2021/file/cbb6a3b884f4f88b3a8e3d44c636cbd8-Paper.pdf) | NeurIPS | 2021 | N/A | `` | |
| [Algorithmic Recourse: from Counterfactual Explanations to Interventions](https://arxiv.org/abs/2002.06278) | FAccT | 2021 | N/A | `` | |
| [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) | CHI | 2021 | N/A | `` | |
| [Explainable Reinforcement Learning via Model Transforms](https://arxiv.org/abs/2209.12006) | NeurIPS | 2021 | N/A | `` | |
| [Aligning Artificial Neural Networks and Ontologies towards Explainable AI](https://ojs.aaai.org/index.php/AAAI/article/view/16626) | AAAI | 2021 | N/A | `` | |

### 2022
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [GlanceNets: Interpretabile, Leak-proof Concept-based Models](https://arxiv.org/abs/2205.15612) | CRL | 2022 | N/A | `` | |
| [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) | Transformer Circuit Thread | 2022 | N/A | `Tutorial` | |
| [Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329) | EMNLP | 2022 | N/A | `DeepMind` | |
| [Interpreting Language Models with Contrastive Explanations](https://arxiv.org/abs/2202.10419) | EMNLP | 2022 | N/A | `` | |
| [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/pdf/2111.09259.pdf) | PNAS | 2022 | N/A | `DeepMind` `GoogleBrain` | |
| [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885) | arXiv | 2022 | [Github](https://github.com/castorini/daam) | `` | |
| [Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis](https://arxiv.org/abs/2106.09992) | AISTATS | 2022 | N/A | `` | |
| [Use-Case-Grounded Simulations for Explanation Evaluation](https://arxiv.org/abs/2206.02256) | NeurIPS | 2022 | N/A | `` | |
| [The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective](https://arxiv.org/abs/2202.01602) | arXiv | 2022 | N/A | `` | |
| [What Makes a Good Explanation?: A Harmonized View of Properties of Explanations](https://arxiv.org/abs/2211.05667) | arXiv | 2022 | N/A | `` | |
| [NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights](https://cdn.aaai.org/ojs/20561/20561-13-24574-1-2-20220628.pdf) | AAAI | 2022 | [Github](https://github.com/understandable-machine-intelligence-lab/NoiseGrad) | `` | |
| [Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations](https://arxiv.org/abs/2205.07277) | AIES (AAAI) | 2022 | N/A | `` | |
| [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models](https://arxiv.org/abs/2202.04053) | arXiv | 2022 | [Github](https://github.com/j-min/DallEval) | `` | |
| [Concept Embedding Models: Beyond the Accuracy-Explainability Trade-Off](https://neurips.cc/Conferences/2022/ScheduleMultitrack?event=52974) | NuerIPS | 2022 | [Github](https://github.com/pietrobarbiero/pytorch_explain) | `CBM`, `CEM` |
| [Self-explaining deep models with logic rule reasoning](https://arxiv.org/abs/2210.07024) | NeurIPS | 2022 | N/A | `` | |
| [What You See is What You Classify: Black Box Attributions](https://arxiv.org/abs/2205.11266) | NeurIPS | 2022 | N/A | `` | |
| [Concept Activation Regions: A Generalized Framework For Concept-Based Explanations](https://arxiv.org/abs/2209.11222) | NeurIPS | 2022 | N/A | `` | |
| [What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods](https://arxiv.org/abs/2112.04417) | NeurIPS | 2022 | N/A | `` | |
| [Scalable Interpretability via Polynomials](https://arxiv.org/abs/2205.14108) | NeurIPS | 2022 | N/A | `` | |
| [Learning to Scaffold: Optimizing Model Explanations for Teaching](https://arxiv.org/abs/2204.10810) | NeurIPS | 2022 | N/A | `` | |
| [Listen to Interpret: Post-hoc Interpretability for Audio Networks with NMF](https://arxiv.org/abs/2202.11479) | NeurIPS | 2022 | N/A | `` | |
| [WeightedSHAP: analyzing and improving Shapley based feature attribution](https://arxiv.org/abs/2209.13429) | NeurIPS | 2022 | N/A | `` | |
| [Visual correspondence-based explanations improve AI robustness and human-AI team accuracy](https://arxiv.org/abs/2208.00780) | NeurIPS | 2022 | N/A | `` | |
| [VICE: Variational Interpretable Concept Embeddings](https://arxiv.org/abs/2205.00756) | NeurIPS | 2022 | N/A | `` | |
| [Robust Feature-Level Adversaries are Interpretability Tools](https://arxiv.org/abs/2110.03605) | NeurIPS | 2022 | N/A | `` | |
| [ProtoX: Explaining a Reinforcement Learning Agent via Prototyping](https://arxiv.org/abs/2211.03162) | NeurIPS | 2022 | N/A | `` | |
| [ProtoVAE: A Trustworthy Self-Explainable Prototypical Variational Model](https://arxiv.org/abs/2210.08151) | NeurIPS | 2022 | N/A | `` | |
| [Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability](https://arxiv.org/abs/2108.01335) | NeurIPS | 2022 | N/A | `` | |
| [Neural Basis Models for Interpretability](https://arxiv.org/abs/2205.14120) | NeurIPS | 2022 | N/A | `` | |
| [Implications of Model Indeterminacy for Explanations of Automated Decisions](https://proceedings.neurips.cc/paper_files/paper/2022/hash/33201f38001dd381aba2c462051449ba-Abstract-Conference.html) | NeurIPS | 2022 | N/A | `` | |
| [Explainability Via Causal Self-Talk](https://openreview.net/pdf?id=bk8vkdQfBS) | NeurIPS | 2022 | N/A | `DeepMind` | |
| [TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations](https://arxiv.org/abs/2207.04154) | NeurIPS | 2022 | N/A | `` | |
| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | NeurIPS | 2022 | N/A | `GoogleBrain` | |
| [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | NeurIPS | 2022 | N/A | `` | |
| [Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations](https://arxiv.org/abs/2206.01254) | NeurIPS | 2022 | N/A | `` | |
| [Foundations of Symbolic Languages for Model Interpretability](https://papers.nips.cc/paper_files/paper/2021/hash/60cb558c40e4f18479664069d9642d5a-Abstract.html) | NeurIPS | 2022 | N/A | `` | |
| [The Utility of Explainable AI in Ad Hoc Human-Machine Teaming](https://papers.nips.cc/paper_files/paper/2021/hash/05d74c48b5b30514d8e9bd60320fc8f6-Abstract.html) | NeurIPS | 2022 | N/A | `` | |
| [Addressing Leakage in Concept Bottleneck Models](https://finale.seas.harvard.edu/sites/scholar.harvard.edu/files/finale/files/10494_addressing_leakage_in_concept_.pdf) | NeurIPS | 2022 | N/A | `` | |
| [Interpreting Language Models with Contrastive Explanations](https://aclanthology.org/2022.emnlp-main.14.pdf) | EMNLP | 2022 | N/A | `` | |
| [Logical Reasoning with Span-Level Predictions for Interpretable and Robust NLI Models](https://aclanthology.org/2022.emnlp-main.251.pdf) | EMNLP | 2022 | N/A | `` | |
| [Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations](https://aclanthology.org/2022.emnlp-main.82.pdf) | EMNLP | 2022 | N/A | `` | |
| [MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure](https://aclanthology.org/2022.emnlp-main.310.pdf) | EMNLP | 2022 | N/A | `` | |
| [Towards Interactivity and Interpretability: A Rationale-based Legal Judgment Prediction Framework](https://aclanthology.org/2022.emnlp-main.316.pdf) | EMNLP | 2022 | N/A | `` | |
| [Explainable Question Answering based on Semantic Graph by Global Differentiable Learning and Dynamic Adaptive Reasoning](https://aclanthology.org/2022.emnlp-main.356.pdf) | EMNLP | 2022 | N/A | `` | |
| [Faithful Knowledge Graph Explanations in Commonsense Question Answering](https://aclanthology.org/2022.emnlp-main.743/) | EMNLP | 2022 | N/A | `` | |
| [Optimal Interpretable Clustering Using Oblique Decision Trees](https://dl.acm.org/doi/pdf/10.1145/3534678.3539361) | KDD | 2022 | N/A | `` | |
| [ExMeshCNN: An Explainable Convolutional Neural Network Architecture for 3D Shape Analysis](https://dl.acm.org/doi/pdf/10.1145/3534678.3539463) | KDD | 2022 | N/A | `` | |
| [Learning Differential Operators for Interpretable Time Series Modeling](https://dl.acm.org/doi/pdf/10.1145/3534678.3539245) | KDD | 2022 | N/A | `` | |
| [Compute Like Humans: Interpretable Step-by-step Symbolic Computation with Deep Neural Network](https://dl.acm.org/doi/10.1145/3534678.3539276) | KDD | 2022 | N/A | `` | |
| [Causal Attention for Interpretable and Generalizable Graph Classification](https://dl.acm.org/doi/10.1145/3534678.3539366) | KDD | 2022 | N/A | `` | |
| [Group-wise Reinforcement Feature Generation for Optimal and Explainable Representation Space Reconstruction](https://dl.acm.org/doi/10.1145/3534678.3539278) | KDD | 2022 | N/A | `` | |
| [Label-Free Explainability for Unsupervised Models](https://proceedings.mlr.press/v162/crabbe22a/crabbe22a.pdf) | ICML | 2022 | N/A | `` | |
| [Rethinking Attention-Model Explainability through Faithfulness Violation Test](https://proceedings.mlr.press/v162/liu22i/liu22i.pdf) | ICML | 2022 | N/A | `` | |
| [Hierarchical Shrinkage: Improving the Accuracy and Interpretability of Tree-Based Methods](https://proceedings.mlr.press/v162/agarwal22b/agarwal22b.pdf) | ICML | 2022 | N/A | `` | |
| [A Functional Information Perspective on Model Interpretation](https://proceedings.mlr.press/v162/gat22a/gat22a.pdf) | ICML | 2022 | N/A | `` | |
| [Inducing Causal Structure for Interpretable Neural Networks](https://proceedings.mlr.press/v162/geiger22a/geiger22a.pdf) | ICML | 2022 | N/A | `` | |
| [ViT-NeT: Interpretable Vision Transformers with Neural Tree Decoder](https://proceedings.mlr.press/v162/kim22g/kim22g.pdf) | ICML | 2022 | N/A | `` | |
| [Interpretable Neural Networks with Frank-Wolfe: Sparse Relevance Maps and Relevance Orderings](https://proceedings.mlr.press/v162/macdonald22a/macdonald22a.pdf) | ICML | 2022 | N/A | `` | |
| [Interpretable and Generalizable Graph Learning via Stochastic Attention Mechanism](https://proceedings.mlr.press/v162/miao22a/miao22a.pdf) | ICML | 2022 | N/A | `` | |
| [Unraveling Attention via Convex Duality: Analysis and Interpretations of Vision Transformers](https://arxiv.org/abs/2205.08078) | ICML | 2022 | N/A | `` | |
| [Robust Models Are More Interpretable Because Attributions Look Normal](https://proceedings.mlr.press/v162/wang22e/wang22e.pdf) | ICML | 2022 | N/A | `` | |
| [Latent Diffusion Energy-Based Model for Interpretable Text Modelling](https://proceedings.mlr.press/v162/yu22h/yu22h.pdf) | ICML | 2022 | N/A | `` | |
| [Crowd, Expert & AI: A Human-AI Interactive Approach Towards Natural Language Explanation based COVID-19 Misinformation Detection](https://www.ijcai.org/proceedings/2022/0706.pdf) | IJCAI | 2022 | N/A | `` | |
| [AttExplainer: Explain Transformer via Attention by Reinforcement Learning](https://www.ijcai.org/proceedings/2022/0102.pdf) | IJCAI | 2022 | N/A | `` | |
| [Investigating and explaining the frequency bias in classification](https://arxiv.org/abs/2205.03154) | IJCAI | 2022 | N/A | `` | |
| [Counterfactual Interpolation Augmentation (CIA): A Unified Approach to Enhance Fairness and Explainability of DNN](https://www.ijcai.org/proceedings/2022/0103.pdf) | IJCAI | 2022 | N/A | `` | |
| [Axiomatic Foundations of Explainability](https://hal.laas.fr/hal-03702681/document) | IJCAI | 2022 | N/A | `` | |
| [Explaining Soft-Goal Conflicts through Constraint Relaxations](https://www.ijcai.org/proceedings/2022/0634.pdf) | IJCAI | 2022 | N/A | `` | |
| [Robust Interpretable Text Classification against Spurious Correlations Using AND-rules with Negation](https://uia.brage.unit.no/uia-xmlui/handle/11250/3057374) | IJCAI | 2022 | N/A | `` | |
| [Interpretable AMR-Based Question Decomposition for Multi-hop Question Answering](https://arxiv.org/abs/2206.08486) | IJCAI | 2022 | N/A | `` | |
| [Toward Policy Explanations for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2204.12568) | IJCAI | 2022 | N/A | `` | |
| [“My nose is running.” “Are you also coughing?”: Building A Medical Diagnosis Agent with Interpretable Inquiry Logics](https://arxiv.org/abs/2204.13953) | IJCAI | 2022 | N/A | `` | |
| [Model Stealing Defense against Exploiting Information Leak Through the Interpretation of Deep Neural Nets](https://www.ijcai.org/proceedings/2022/0100.pdf) | IJCAI | 2022 | N/A | `` | |
| [Learning by Interpreting](https://www.ijcai.org/proceedings/2022/0609.pdf) | IJCAI | 2022 | N/A | `` | |
| [Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies](https://arxiv.org/abs/2205.01240) | IJCAI | 2022 | N/A | `` | |
| [Explanations for Negative Query Answers under Inconsistency-Tolerant Semantics](https://www.ijcai.org/proceedings/2022/0375.pdf) | IJCAI | 2022 | N/A | `` | |
| [On Preferred Abductive Explanations for Decision Trees and Random Forests](https://hal.science/hal-03764873/) | IJCAI | 2022 | N/A | `` | |
| [Adversarial Explanations for Knowledge Graph Embeddings](https://www.ijcai.org/proceedings/2022/0391.pdf) | IJCAI | 2022 | N/A | `` | |
| [Looking Inside the Black-Box: Logic-based Explanations for Neural Networks](https://proceedings.kr.org/2022/45/) | KR | 2022 | N/A | `` | |
| [Entropy-Based Logic Explanations of Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/20551) | AAAI | 2022 | N/A | `` | |
| [Explainable Neural Rule Learning](https://dl.acm.org/doi/pdf/10.1145/3485447.3512023) | WWW | 2022 | N/A | `` | |
| [Explainable Deep Learning: A Field Guide for the Uninitiated](https://www.jair.org/index.php/jair/article/view/13200) | JAIR | 2022 | N/A | `` | |
| []() |  |  | N/A | `` | |


### 2023
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [On the Privacy Risks of Algorithmic Recourse](https://arxiv.org/abs/2211.05427) | AISTATS | 2023 | N/A | `` | |
| [Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten](https://arxiv.org/abs/2302.04288) | ICML | 2023 | N/A | `` | |
| [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062) | arXiv | 2023 | [Github](https://github.com/deepmind/tracr) | `DeepMind` | |
| [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse](https://arxiv.org/abs/2203.06768) | ICLR | 2023 | N/A | `` | |
| [Concept-level Debugging of Part-Prototype Networks](https://openreview.net/forum?id=oiwXWPDTyNk) | ICLR | 2023 | N/A | `` | |
| [Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning](https://arxiv.org/abs/2203.16464) | ICLR | 2023 | N/A | `` | |
| [Re-calibrating Feature Attributions for Model Interpretation](https://openreview.net/pdf?id=WUWJIV2Yxtp) | ICLR | 2023 | N/A | `` | |
| [Post-hoc Concept Bottleneck Models](https://arxiv.org/abs/2205.15480) | ICLR | 2023 | N/A | `` | |
| [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646) | ICLR | 2023 | N/A | `` | |
| [STREET: A Multi-Task Structured Reasoning and Explanation Benchmark](https://arxiv.org/abs/2302.06729) | ICLR | 2023 | N/A | `` | |
| [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | |
| [EVAL: Explainable Video Anomaly Localization](https://openaccess.thecvf.com/content/CVPR2023/papers/Singh_EVAL_Explainable_Video_Anomaly_Localization_CVPR_2023_paper.pdf) | CVPR | 2023 | N/A | `` | |
| [Overlooked Factors in Concept-based Explanations: Dataset Choice, Concept Learnability, and Human Capability](https://arxiv.org/abs/2207.09615) | CVPR | 2023 | [Github](https://github.com/princetonvisualai/OverlookedFactors) | `` | |
| [Spatial-Temporal Concept Based Explanation of 3D ConvNets](https://arxiv.org/abs/2206.05275) | CVPR | 2023 | [Github](https://github.com/yingji425/STCE) | `` | |
| [Adversarial Counterfactual Visual Explanations](https://arxiv.org/abs/2303.09962) | CVPR | 2023 | N/A | `` | |
| [Bridging the Gap Between Model Explanations in Partially Annotated Multi-Label Classification](https://arxiv.org/abs/2304.01804) | CVPR | 2023 | N/A | `` | |
| [Explaining Image Classifiers With Multiscale Directional Image Representation](https://arxiv.org/abs/2211.12857) | CVPR | 2023 | N/A | `` | |
| [CRAFT: Concept Recursive Activation FacTorization for Explainability](https://arxiv.org/abs/2211.10154) | CVPR | 2023 | N/A | `` | |
| [SketchXAI: A First Look at Explainability for Human Sketches](https://arxiv.org/abs/2304.11744) | CVPR | 2023 | N/A | `` | |
| [Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis](https://arxiv.org/abs/2202.07728) | CVPR | 2023 | N/A | `` | |
| [Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning](https://arxiv.org/abs/2304.04824) | CVPR | 2023 | N/A | `` | |
| [Learning Bottleneck Concepts in Image Classification](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Learning_Bottleneck_Concepts_in_Image_Classification_CVPR_2023_paper.html) | CVPR | 2023 | N/A | `` | |
| [Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification](https://arxiv.org/abs/2211.11158) | CVPR | 2023 | N/A | `` | |
| [Interpretable Neural-Symbolic Concept Reasoning](https://arxiv.org/pdf/2304.14068.pdf) | ICML | 2023 | [Github](https://github.com/pietrobarbiero/pytorch_explain)                                                            |         | |
| [Identifying Interpretable Subspaces in Image Representations](https://openreview.net/pdf?id=5YUyJYElAc) | ICML | 2023 | N/A | `` | |
| [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat](https://openreview.net/pdf?id=0SgBUsL4W0) | ICML | 2023 | N/A | `` | |
| [Explainability as statistical inference](https://openreview.net/pdf?id=RPzQOi1Cyf) | ICML | 2023 | N/A | `` | |
| [On the Impact of Knowledge Distillation for Model Interpretability](https://openreview.net/pdf?id=XOTFW2BK6i) | ICML | 2023 | N/A | `` | |
| [NA2Q: Neural Attention Additive Model for Interpretable Multi-Agent Q-Learning](https://openreview.net/pdf?id=oUeo2uG1AZ) | ICML | 2023 | N/A | `` | |
| [Explaining Reinforcement Learning with Shapley Values](https://openreview.net/pdf?id=R1blujRwj1) | ICML | 2023 | N/A | `` | |
| [Explainable Data-Driven Optimization: From Context to Decision and Back Again](https://openreview.net/pdf?id=4Lk9GHHueJ) | ICML | 2023 | N/A | `` | |
| [Causal Proxy Models for Concept-based Model Explanations](https://openreview.net/pdf?id=1Hh1cIPJ7V) | ICML | 2023 | N/A | `` | |
| [Learning Perturbations to Explain Time Series Predictions](https://openreview.net/pdf?id=WpeZu6WzTB) | ICML | 2023 | N/A | `` | |
| [Rethinking Explaining Graph Neural Networks via Non-parametric Subgraph Matching](https://openreview.net/pdf?id=MocsSAUKlk) | ICML | 2023 | N/A | `` | |
| [Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat](https://openreview.net/pdf?id=0SgBUsL4W0) | ICML | 2023 | [Github](https://github.com/batmanlab/ICML-2023-Route-interpret-repeat) | `` | |
| [Representer Point Selection for Explaining Regularized High-dimensional Models](https://openreview.net/pdf?id=GLI2hX4vxx) | ICML | 2023 | N/A | `` | |
| [Towards Explaining Distribution Shifts](https://openreview.net/pdf?id=Tig5ELxc0M) | ICML | 2023 | N/A | `` | |
| [Relevant Walk Search for Explaining Graph Neural Networks](https://openreview.net/pdf?id=BDYIci7bVs) | ICML | 2023 | [Github](https://github.com/xiong-ping/rel_walk_gnnlrp) | `` | |
| [Concept-based Explanations for Out-of-Distribution Detectors](https://openreview.net/pdf?id=a33IYBCFey) | ICML | 2023 | N/A | `` | |
| [GLOBE-CE: A Translation Based Approach for Global Counterfactual Explanations](https://openreview.net/pdf?id=KHqQwzx2H2) | ICML | 2023 | [Github](https://github.com/danwley/GLOBE-CE/) | `` | |
| [Robust Explanation for Free or At the Cost of Faithfulness](https://openreview.net/pdf?id=6bfF0RYvMy) | ICML | 2023 | N/A | `` | |
| [Learn to Accumulate Evidence from All Training Samples: Theory and Practice](https://openreview.net/pdf?id=2MaUpKBSju) | ICML | 2023 | N/A | `` | |
| [Towards Trustworthy Explanation: On Causal Rationalization](https://openreview.net/pdf?id=fvTgh4MNUV) | ICML | 2023 | N/A | `` | |
| [Theoretical Behavior of XAI Methods in the Presence of Suppressor Variables](https://openreview.net/pdf?id=BdwGV6fwbK) | ICML | 2023 | N/A | `` | |
| [Probabilistic Concept Bottleneck Models](https://openreview.net/pdf?id=yOxy3T0d6e) | ICML | 2023 | N/A | `` | |
| [What do CNNs Learn in the First Layer and Why? A Linear Systems Perspective](https://openreview.net/pdf?id=RJGad2VFYk) | ICML | 2023 | N/A | `` | |
| [Towards credible visual model interpretation with path attribution](https://openreview.net/pdf?id=cHZBCZmfSo) | ICML | 2023 | N/A | `` | |
| [Trainability, Expressivity and Interpretability in Gated Neural ODEs](https://openreview.net/pdf?id=ZhO8woi9CX) | ICML | 2023 | N/A | `` | |
| [Discover and Cure: Concept-aware Mitigation of Spurious Correlation](https://openreview.net/pdf?id=QDxtrlPmfB) | ICML | 2023 | N/A | `` | |
| [PWSHAP: A Path-Wise Explanation Model for Targeted Variables](https://openreview.net/pdf?id=u8VEJNykA5) | ICML | 2023 | N/A | `` | |
| [A Closer Look at the Intervention Procedure of Concept Bottleneck Models](https://openreview.net/pdf?id=YIWtM3GdZc) | ICML | 2023 | N/A | `` | |
| [Counterfactual Analysis in Dynamic Latent-State Models](https://proceedings.mlr.press/v202/haugh23a/haugh23a.pdf) | ICML | 2023 | N/A | `` | |
| [Tackling Shortcut Learning in Deep Neural Networks: An Iterative Approach with Interpretable Models](https://openreview.net/pdf?id=m5vnLHfNy7) | ICML Workshop | 2023 | N/A | `` | |
| [Rethinking Interpretation: Input-Agnostic Saliency Mapping of Deep Visual Classifiers](https://ojs.aaai.org/index.php/AAAI/article/view/25089) | AAAI | 2023 | N/A | `` | |
| [TopicFM: Robust and Interpretable Topic-Assisted Feature Matching](https://ojs.aaai.org/index.php/AAAI/article/view/25341) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [PEN: Prediction-Explanation Network to Forecast Stock Price Movement with Better Explainability](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [KerPrint: Local-Global Knowledge Graph Enhanced Diagnosis Prediction for Retrospective and Prospective Interpretations](https://ojs.aaai.org/index.php/AAAI/article/view/25667) | AAAI | 2023 | N/A | `` | |
| [Beyond Graph Convolutional Network: An Interpretable Regularizer-Centered Optimization Framework](https://ojs.aaai.org/index.php/AAAI/article/view/25593) | AAAI | 2023 | N/A | `` | |
| [Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling](https://ojs.aaai.org/index.php/AAAI/article/view/25812) | AAAI | 2023 | N/A | `` | |
| [Learning Interpretable Temporal Properties from Positive Examples Only](https://ojs.aaai.org/index.php/AAAI/article/view/25800) | AAAI | 2023 | N/A | `` | |
| [Symbolic Metamodels for Interpreting Black-Boxes Using Primitive Functions](https://ojs.aaai.org/index.php/AAAI/article/view/25816) | AAAI | 2023 | N/A | `` | |
| [Towards More Robust Interpretation via Local Gradient Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/25986) | AAAI | 2023 | N/A | `` | |
| [Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network](https://ojs.aaai.org/index.php/AAAI/article/view/26040) | AAAI | 2023 | N/A | `` | |
| [XClusters: Explainability-First Clustering](https://ojs.aaai.org/index.php/AAAI/article/view/25963) | AAAI | 2023 | N/A | `` | |
| [Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/26267) | AAAI | 2023 | N/A | `` | |
| [Fairness and Explainability: Bridging the Gap towards Fair Model Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/26344) | AAAI | 2023 | N/A | `` | |
| [Explaining Model Confidence Using Counterfactuals](https://ojs.aaai.org/index.php/AAAI/article/view/26399) | AAAI | 2023 | N/A | `` | |
| [SEAT: Stable and Explainable Attention](https://ojs.aaai.org/index.php/AAAI/article/view/26517) | AAAI | 2023 | N/A | `` | |
| [Factual and Informative Review Generation for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26618) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability via Explicit Word Interaction Graph Layer](https://ojs.aaai.org/index.php/AAAI/article/view/26586) | AAAI | 2023 | N/A | `` | |
| [Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing](https://ojs.aaai.org/index.php/AAAI/article/view/26572) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations](https://ojs.aaai.org/index.php/AAAI/article/view/26661) | AAAI | 2023 | N/A | `` | |
| [Targeted Knowledge Infusion To Make Conversational AI Explainable and Safe](https://ojs.aaai.org/index.php/AAAI/article/view/26805) | AAAI | 2023 | N/A | `` | |
| [eForecaster: Unifying Electricity Forecasting with Robust, Flexible, and Explainable Machine Learning Algorithms](https://ojs.aaai.org/index.php/AAAI/article/view/26853) | AAAI | 2023 | N/A | `` | |
| [SolderNet: Towards Trustworthy Visual Inspection of Solder Joints in Electronics Manufacturing Using Explainable Artificial Intelligence](https://ojs.aaai.org/index.php/AAAI/article/view/26858) | AAAI | 2023 | N/A | `` | |
| [Xaitk-Saliency: An Open Source Explainable AI Toolkit for Saliency](https://ojs.aaai.org/index.php/AAAI/article/view/26871) | AAAI | 2023 | N/A | `` | |
| [Ripple: Concept-Based Interpretation for Raw Time Series Models in Education](https://ojs.aaai.org/index.php/AAAI/article/view/26888) | AAAI | 2023 | N/A | `` | |
| [Semantics, Ontology and Explanation](https://arxiv.org/abs/2304.11124) | arXiv | 2023 | N/A | `Ontological Unpacking` | |
| [Post Hoc Explanations of Language Models Can Improve Language Models](https://arxiv.org/pdf/2305.11426.pdf) | arXiv | 2023 | N/A | `` | |
| [TopicFM: Robust and Interpretable Topic-Assisted Feature Matching](https://ojs.aaai.org/index.php/AAAI/article/view/25341) | AAAI | 2023 | N/A | `` | |
| [Beyond Graph Convolutional Network: An Interpretable Regularizer-Centered Optimization Framework](https://ojs.aaai.org/index.php/AAAI/article/view/25593) | AAAI | 2023 | N/A | `` | |
| [KerPrint: Local-Global Knowledge Graph Enhanced Diagnosis Prediction for Retrospective and Prospective Interpretations](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [PEN: Prediction-Explanation Network to Forecast Stock Price Movement with Better Explainability](https://ojs.aaai.org/index.php/AAAI/article/view/25648) | AAAI | 2023 | N/A | `` | |
| [Solving Explainability Queries with Quantification: The Case of Feature Relevancy](https://ojs.aaai.org/index.php/AAAI/article/view/25514) | AAAI | 2023 | N/A | `` | |
| [Multi-Aspect Explainable Inductive Relation Prediction by Sentence Transformer](https://ojs.aaai.org/index.php/AAAI/article/view/25803) | AAAI | 2023 | N/A | `` | |
| [Learning to Select Prototypical Parts for Interpretable Sequential Data Modeling](https://ojs.aaai.org/index.php/AAAI/article/view/25812) | AAAI | 2023 | N/A | `` | |
| [Learning Interpretable Temporal Properties from Positive Examples Only](https://ojs.aaai.org/index.php/AAAI/article/view/25800) | AAAI | 2023 | N/A | `` | |
| [Unfooling Perturbation-Based Post Hoc Explainers](https://ojs.aaai.org/index.php/AAAI/article/view/25847) | AAAI | 2023 | N/A | `` | |
| [Very Fast, Approximate Counterfactual Explanations for Decision Forests](https://ojs.aaai.org/index.php/AAAI/article/view/25848) | AAAI | 2023 | N/A | `` | |
| [Symbolic Metamodels for Interpreting Black-Boxes Using Primitive Functions](https://ojs.aaai.org/index.php/AAAI/article/view/25816) | AAAI | 2023 | N/A | `` | |
| [Towards More Robust Interpretation via Local Gradient Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/25986) | AAAI | 2023 | N/A | `` | |
| [Towards Fine-Grained Explainability for Heterogeneous Graph Neural Network](https://ojs.aaai.org/index.php/AAAI/article/view/26040) | AAAI | 2023 | N/A | `` | |
| [Local Explanations for Reinforcement Learning](https://ojs.aaai.org/index.php/AAAI/article/view/26081) | AAAI | 2023 | N/A | `` | |
| [ConceptX: A Framework for Latent Concept Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/27057) | AAAI | 2023 | N/A | `` | |
| [XClusters: Explainability-First Clustering](https://ojs.aaai.org/index.php/AAAI/article/view/25963) | AAAI | 2023 | N/A | `` | |
| [Explaining Random Forests Using Bipolar Argumentation and Markov Networks](https://ojs.aaai.org/index.php/AAAI/article/view/26132) | AAAI | 2023 | N/A | `` | |
| [Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis](https://ojs.aaai.org/index.php/AAAI/article/view/26267) | AAAI | 2023 | N/A | `` | |
| [Fairness and Explainability: Bridging the Gap towards Fair Model Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/26344) | AAAI | 2023 | N/A | `` | |
| [Explaining Model Confidence Using Counterfactuals](https://ojs.aaai.org/index.php/AAAI/article/view/26399) | AAAI | 2023 | N/A | `` | |
| [XRand: Differentially Private Defense against Explanation-Guided Attacks](https://ojs.aaai.org/index.php/AAAI/article/view/26401) | AAAI | 2023 | N/A | `` | |
| [Unsupervised Explanation Generation via Correct Instantiations](https://ojs.aaai.org/index.php/AAAI/article/view/26494) | AAAI | 2023 | N/A | `` | |
| [SEAT: Stable and Explainable Attention](https://ojs.aaai.org/index.php/AAAI/article/view/26517) | AAAI | 2023 | N/A | `` | |
| [Disentangled CVAEs with Contrastive Learning for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26604) | AAAI | 2023 | N/A | `` | |
| [Factual and Informative Review Generation for Explainable Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/26618) | AAAI | 2023 | N/A | `` | |
| [Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing](https://ojs.aaai.org/index.php/AAAI/article/view/26572) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability via Explicit Word Interaction Graph Layer](https://ojs.aaai.org/index.php/AAAI/article/view/26586) | AAAI | 2023 | N/A | `` | |
| [Improving Interpretability of Deep Sequential Knowledge Tracing Models with Question-centric Cognitive Representations](https://ojs.aaai.org/index.php/AAAI/article/view/26661) | AAAI | 2023 | N/A | `` | |
| [Interpretable Chirality-Aware Graph Neural Network for Quantitative Structure Activity Relationship Modeling in Drug Discovery](https://ojs.aaai.org/index.php/AAAI/article/view/26679) | AAAI | 2023 | N/A | `` | |
| [Monitoring Model Deterioration with Explainable Uncertainty Estimation via Non-parametric Bootstrap](https://ojs.aaai.org/index.php/AAAI/article/view/26755) | AAAI | 2023 | N/A | `` | |
| [Interactive Concept Bottleneck Models](https://ojs.aaai.org/index.php/AAAI/article/view/25736/25508) | AAAI | 2023 | N/A | `` | |
| [Data-Efficient and Interpretable Tabular Anomaly Detection](https://arxiv.org/abs/2203.02034) | KDD | 2023 | N/A | `` | |
| [Counterfactual Learning on Heterogeneous Graphs with Greedy Perturbation](https://dl.acm.org/doi/pdf/10.1145/3580305.3599289) | KDD | 2023 | N/A | `` | |
| [Hands-on Tutorial: "Explanations in AI: Methods, Stakeholders and Pitfalls"](https://dl.acm.org/doi/abs/10.1145/3580305.3599181) | KDD | 2023 | N/A | `` | |
| [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599343) | KDD | 2023 | N/A | `` | |
| [Generative AI meets Responsible AI: Practical Challenges and Opportunities](https://dl.acm.org/doi/abs/10.1145/3580305.3599557) | KDD | 2023 | N/A | `` | |
| [Empower Post-hoc Graph Explanations with Information Bottleneck: A Pre-training and Fine-tuning Perspective](https://dl.acm.org/doi/10.1145/3580305.3599330) | KDD | 2023 | N/A | `` | |
| [MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599435) | KDD | 2023 | N/A | `` | |
| [CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599290) | KDD | 2023 | N/A | `` | |
| [Fire: An Optimization Approach for Fast Interpretable Rule Extraction](https://dl.acm.org/doi/10.1145/3580305.3599353) | KDD | 2023 | N/A | `` | |
| [ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599336) | KDD | 2023 | N/A | `` | |
| [A Causality Inspired Framework for Model Interpretation](https://dl.acm.org/doi/10.1145/3580305.3599240) | KDD | 2023 | N/A | `` | |
| [Path-Specific Counterfactual Fairness for Recommender Systems](https://dl.acm.org/doi/abs/10.1145/3580305.3599462) | KDD | 2023 | N/A | `` | |
| [SURE: Robust, Explainable, and Fair Classification without Sensitive Attributes](https://dl.acm.org/doi/10.1145/3580305.3599514) | KDD | 2023 | N/A | `` | |
| [Learning for Counterfactual Fairness from Observational Data](https://dl.acm.org/doi/10.1145/3580305.3599408) | KDD | 2023 | N/A | `` | |
| [Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks](https://dl.acm.org/doi/abs/10.1145/3580305.3599394) | KDD | 2023 | N/A | `` | |
| [ExplainableFold: Understanding AlphaFold Prediction with Explainable AI](https://dl.acm.org/doi/abs/10.1145/3580305.3599337) | KDD | 2023 | N/A | `` | |
| [FLAMES2Graph: An Interpretable Federated Multivariate Time Series Classification Framework](https://dl.acm.org/doi/abs/10.1145/3580305.3599354) | KDD | 2023 | N/A | `` | |
| [Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations](https://dl.acm.org/doi/abs/10.1145/3580305.3599343) | KDD | 2023 | N/A | `` | |
| [ESSA: Explanation Iterative Supervision via Saliency-guided Data Augmentation](https://dl.acm.org/doi/abs/10.1145/3580305.3599336) | KDD | 2023 | N/A | `` | |
| [Counterfactual Explanations and Model Multiplicity: a Relational Verification View](https://proceedings.kr.org/2023/78/kr2023-0078-leofante-et-al.pdf) | Proceedings of KR | 2023 | N/A | `` | |
| [Explainable Representations for Relation Prediction in Knowledge Graphs](https://proceedings.kr.org/2023/62/) | Proceedings of KR | 2023 | N/A | `` | |
| [Region-based Saliency Explanations on the Recognition of Facial Genetic Syndromes](https://static1.squarespace.com/static/59d5ac1780bd5ef9c396eda6/t/64d1a2c33e2b3e04b8de3b24/1691460293298/ID97_Research+Paper_2023.pdf) | PMLR | 2023 | N/A | `` | |
| [FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods](https://arxiv.org/pdf/2308.06248.pdf) | arXiv | 2023 | N/A | `` | |
| [Diffusion-based Visual Counterfactual Explanations - Towards Systematic Quantitative Evaluation](https://arxiv.org/pdf/2308.06100.pdf) | arXiv | 2023 | N/A | `` | |
| [Testing methods of neural systems understanding](https://www.sciencedirect.com/science/article/pii/S1389041723000906) | Cognitive Systems Research | 2023 | N/A | `` | |
| [Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning](https://arxiv.org/pdf/2308.03999.pdf) | arXiv | 2023 | N/A | `` | |
| [An Explainable Federated Learning and Blockchain based Secure Credit Modeling Method](https://www.sciencedirect.com/science/article/abs/pii/S0377221723006677) | EJOR | 2023 | N/A | `` | |
| [i-Align: an interpretable knowledge graph alignment model](https://link.springer.com/article/10.1007/s10618-023-00963-3) | DMKD | 2023 | N/A | `` | |
| [Goodhart’s Law Applies to NLP’s Explanation Benchmarks](https://arxiv.org/pdf/2308.14272.pdf) | arXiv | 2023 | N/A | `` | |
| [DELELSTM: DECOMPOSITION-BASED LINEAR EXPLAINABLE LSTM TO CAPTURE INSTANTANEOUS AND LONG-TERM EFFECTS IN TIME SERIES](https://arxiv.org/pdf/2308.13797.pdf) | arXiv | 2023 | N/A | `` | |
| [BEYOND DISCRIMINATIVE REGIONS: SALIENCY MAPS AS ALTERNATIVES TO CAMS FOR WEAKLY SU- PERVISED SEMANTIC SEGMENTATION](https://arxiv.org/pdf/2308.11052.pdf) | arXiv | 2023 | N/A | `` | |
| [SEA: Shareable and Explainable Attribution for Query-based Black-box Attacks](https://arxiv.org/pdf/2308.11845.pdf) | arXiv | 2023 | N/A | `` | |
| [Sparse Linear Concept Discovery Models](https://arxiv.org/pdf/2308.10782.pdf) | arXiv | 2023 | N/A | `` | |
| [Revisiting the Performance-Explainability Trade-Off in Explainable Artificial Intelligence (XAI)](https://arxiv.org/pdf/2307.14239.pdf) | arXiv | 2023 | N/A | `` | |
| [KGTN: Knowledge Graph Transformer Network for explainable multi-category item recommendation](https://www.sciencedirect.com/science/article/abs/pii/S0950705123006044) | KBS | 2023 | N/A | `` | |
| [SAFE: Saliency-Aware Counterfactual Explanations for DNN-based Automated Driving Systems](https://arxiv.org/pdf/2307.15786.pdf) | arXiv | 2023 | N/A | `` | |
| [Explainable Multi-Agent Reinforcement Learning for Temporal Queries](https://www.ijcai.org/proceedings/2023/0007.pdf) | IJCAI | 2023 | N/A | `` | |
| [Advancing Post-Hoc Case-Based Explanation with Feature Highlighting](https://www.ijcai.org/proceedings/2023/0048.pdf) | IJCAI | 2023 | N/A | `` | |
| [Explanation-Guided Reward Alignment](https://www.ijcai.org/proceedings/2023/0053.pdf) | IJCAI | 2023 | N/A | `` | |
| [FEAMOE: Fair, Explainable and Adaptive Mixture of Experts](https://www.ijcai.org/proceedings/2023/0055.pdf) | IJCAI | 2023 | N/A | `` | |
| [Statistically Significant Concept-based Explanation of Image Classifiers via Model Knockoffs](https://www.ijcai.org/proceedings/2023/0058.pdf) | IJCAI | 2023 | N/A | `` | |
| [Learning Prototype Classifiers for Long-Tailed Recognition](https://www.ijcai.org/proceedings/2023/0151.pdf) | IJCAI | 2023 | N/A | `` | |
| [On Translations between ML Models for XAI Purposes](https://www.ijcai.org/proceedings/2023/0352.pdf) | IJCAI | 2023 | N/A | `` | |
| [The Parameterized Complexity of Finding Concise Local Explanations](https://www.ijcai.org/proceedings/2023/0369.pdf) | IJCAI | 2023 | N/A | `` | |
| [Neuro-Symbolic Class Expression Learning](https://www.ijcai.org/proceedings/2023/0403.pdf) | IJCAI | 2023 | N/A | `` | |
| [A Logic-based Approach to Contrastive Explainability for Neurosymbolic Visual Question Answering](https://www.ijcai.org/proceedings/2023/0408.pdf) | IJCAI | 2023 | N/A | `` | |
| [Cardinality-Minimal Explanations for Monotonic Neural Networks](https://www.ijcai.org/proceedings/2023/0409.pdf) | IJCAI | 2023 | N/A | `` | |
| [Unveiling Concepts Learned by a World-Class Chess-Playing Agent](https://www.ijcai.org/proceedings/2023/0541.pdf) | IJCAI | 2023 | N/A | `` | |
| [Explainable Text Classification via Attentive and Targeted Mixing Data Augmentation](https://www.ijcai.org/proceedings/2023/0565.pdf) | IJCAI | 2023 | N/A | `` | |
| [On the Complexity of Counterfactual Reasoning](https://www.ijcai.org/proceedings/2023/0630.pdf) | IJCAI | 2023 | N/A | `` | |
| [Interpretable Local Concept-based Explanation with Human Feedback to Predict All-cause Mortality (Extended Abstract)](https://www.ijcai.org/proceedings/2023/0774.pdf) | IJCAI | 2023 | N/A | `` | |
| [Good-looking but Lacking Faithfulness: Understanding Local Explanation Methods through Trend-based Testing](https://arxiv.org/pdf/2309.05679.pdf) | arXiv | 2023 | N/A | `` | |
| [Counterfactual Explanations via Locally-guided Sequential Algorithmic Recourse](https://arxiv.org/pdf/2309.04211.pdf) | arXiv | 2023 | N/A | `` | |
| [Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations](https://arxiv.org/pdf/2309.04676.pdf) | CIKM | 2023 | N/A | `` | |
| [A Function Interpretation Benchmark for Evaluating Interpretability Methods](https://arxiv.org/pdf/2309.03886.pdf) | arXiv | 2023 | N/A | `` | |
| [Explaining through Transformer Input Sampling](https://dial.uclouvain.be/pr/boreal/object/boreal%3A277791/datastream/PDF_01/view) | arXiv | 2023 | N/A | `` | |
| [Backtracking Counterfactuals](https://proceedings.mlr.press/v213/kugelgen23a/kugelgen23a.pdf) | CLeaR | 2023 | N/A | `` | |
| [Text2Concept: Concept Activation Vectors Directly from Text](https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Moayeri_Text2Concept_Concept_Activation_Vectors_Directly_From_Text_CVPRW_2023_paper.pdf) | CVPR Workshop | 2023 | N/A | `` | |
| [A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation](https://arxiv.org/pdf/2306.07304.pdf) | arXiv | 2023 | N/A | `` | |
| [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance](https://arxiv.org/pdf/2304.06715.pdf) | NeurIPS | 2023 | [Github](https://github.com/JonathanCrabbe/RobustXAI) | `` | |
| [CLIP-DISSECT: AUTOMATIC DESCRIPTION OF NEU- RON REPRESENTATIONS IN DEEP VISION NETWORKS](https://openreview.net/pdf?id=iPWiwWHc1V) | ICLR | 2023 | [Github](https://github.com/Trustworthy-ML-Lab/CLIP-dissect/tree/main) | `` | |
| [Label-free Concept Bottleneck Models](https://openreview.net/pdf?id=FlCg47MNvBA) | ICLR | 2023 | N/A | `` | |
| [Concept-level Debugging of Part-Prototype Networks](https://openreview.net/forum?id=oiwXWPDTyNk) | ICLR | 2023 | N/A | `` | |
| [Towards Interpretable Deep Reinforcement Learning with Human-Friendly Prototypes](https://openreview.net/forum?id=hWwY_Jq0xsN) | ICLR | 2023 | N/A | `` | |
| [Re-calibrating Feature Attributions for Model Interpretation](https://openreview.net/forum?id=WUWJIV2Yxtp) | ICLR | 2023 | N/A | `` | |
| [Post-hoc Concept Bottleneck Models ](https://openreview.net/forum?id=nA5AZ8CEyow) | ICLR | 2023 | N/A | `` | |
| [Information Maximization Perspective of Orthogonal Matching Pursuit with Applications to Explainable AI](https://papers.nips.cc/paper_files/paper/2023/hash/08eac13583b310ec55d755f99c549be3-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Explaining Predictive Uncertainty with Information Theoretic Shapley Values](https://papers.nips.cc/paper_files/paper/2023/hash/16e4be78e61a3897665fa01504e9f452-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths](https://papers.nips.cc/paper_files/paper/2023/hash/2ebf43d20e5933ab6d98225bbb908ade-Abstract-Datasets_and_Benchmarks.html) | NeurIPS | 2023 | N/A | `` | |
| [Explain Any Concept: Segment Anything Meets Concept-Based Explanation](https://papers.nips.cc/paper_files/paper/2023/hash/44cdeb5ab7da31d9b5cd88fd44e3da84-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [VeriX: Towards Verified Explainability of Deep Neural Networks](https://papers.nips.cc/paper_files/paper/2023/hash/46907c2ff9fafd618095161d76461842-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Explainable and Efficient Randomized Voting Rules](https://papers.nips.cc/paper_files/paper/2023/hash/47eb2874a790d5b1f554b9bb93b3de9d-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery](https://papers.nips.cc/paper_files/paper/2023/hash/5c5bc3553815adb4d1a8a5b8701e41a9-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Explaining the Uncertain: Stochastic Shapley Values for Gaussian Process Models](https://papers.nips.cc/paper_files/paper/2023/hash/9f0b1220028dfa2ee82ca0a0e0fc52d1-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [V-InFoR: A Robust Graph Neural Networks Explainer for Structurally Corrupted Graphs](https://papers.nips.cc/paper_files/paper/2023/hash/b07d36fb3fae0630897700593c8cf49d-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Explainable Brain Age Prediction using coVariance Neural Networks](https://papers.nips.cc/paper_files/paper/2023/hash/92bb2145c74b7d10fbb61aba315b5010-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [TempME: Towards the Explainability of Temporal Graph Neural Networks via Motif Discovery](https://papers.nips.cc/paper_files/paper/2023/hash/5c5bc3553815adb4d1a8a5b8701e41a9-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [D4Explainer: In-distribution Explanations of Graph Neural Network via Discrete Denoising Diffusion](https://papers.nips.cc/paper_files/paper/2023/hash/f978c8f3b5f399cae464e85f72e28503-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [StateMask: Explaining Deep Reinforcement Learning through State Mask](https://papers.nips.cc/paper_files/paper/2023/hash/c4bf73386022473a652a18941e9ea6f8-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [LICO: Explainable Models with Language-Image COnsistency](https://papers.nips.cc/paper_files/paper/2023/hash/c2eac51b6353a4441e8b7426f8e8db78-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [On the explainable properties of 1-Lipschitz Neural Networks: An Optimal Transport Perspective](https://papers.nips.cc/paper_files/paper/2023/hash/ab5a2bf4385bee44f3919060b184605b-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction](https://papers.nips.cc/paper_files/paper/2023/hash/9f42f06a54ce3b709ad78d34c73e4363-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Discriminative Feature Attributions: Bridging Post Hoc Explainability and Inherent Interpretability](https://papers.nips.cc/paper_files/paper/2023/hash/89beb2a345269f3f9afe48cee35403aa-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Train Once and Explain Everywhere: Pre-training Interpretable Graph Neural Networks](https://papers.nips.cc/paper_files/paper/2023/hash/6ecd51685e2d765bc0ad32a2e73faf62-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples](https://papers.nips.cc/paper_files/paper/2023/hash/096b1019463f34eb241e87cfce8dfe16-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [HiBug: On Human-Interpretable Model Debug](https://papers.nips.cc/paper_files/paper/2023/hash/0f53ecc0d36a5d5d3d3e94d42c4b23ca-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Towards Self-Interpretable Graph-Level Anomaly Detection](https://papers.nips.cc/paper_files/paper/2023/hash/1c6f06863df46de009a7a41b41c95cad-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpretable Graph Networks Formulate Universal Algebra Conjectures](https://papers.nips.cc/paper_files/paper/2023/hash/2b2011a7d5396faf5899863d896a3c24-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Towards Automated Circuit Discovery for Mechanistic Interpretabilit](https://papers.nips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpretable Reward Redistribution in Reinforcement Learning: A Causal Approach](https://papers.nips.cc/paper_files/paper/2023/hash/402e12102d6ec3ea3df40ce1b23d423a-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [DISCOVER: Making Vision Networks Interpretable via Competition and Dissection](https://papers.nips.cc/paper_files/paper/2023/hash/55aeba84b402008d3ed10440d906b4e1-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [MultiMoDN—Multimodal, Multi-Task, Interpretable Modular Networks](https://papers.nips.cc/paper_files/paper/2023/hash/5951641ad71b0052cf776f9b71f18932-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Causal Interpretation of Self-Attention in Pre-Trained Transformers](https://papers.nips.cc/paper_files/paper/2023/hash/642a321fba8a0f03765318e629cb93ea-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://papers.nips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Learning Interpretable Low-dimensional Representation via Physical Symmetry](https://papers.nips.cc/paper_files/paper/2023/hash/9850e6a5410331290dc1deefb7514448-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Scale Alone Does not Improve Mechanistic Interpretability in Vision Models](https://papers.nips.cc/paper_files/paper/2023/hash/b4aadf04d6fde46346db455402860708-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Transitivity Recovering Decompositions: Interpretable and Robust Fine-Grained Relationships](https://papers.nips.cc/paper_files/paper/2023/hash/beba7cfdac084a0f53f378d42cbe2824-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [GRAND-SLAMIN’ Interpretable Additive Modeling with Structural Constraints](https://papers.nips.cc/paper_files/paper/2023/hash/c057cb81b8d3c67093427bf1c16a4e9f-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpreting Unsupervised Anomaly Detection in Security via Rule Extraction](https://papers.nips.cc/paper_files/paper/2023/hash/c43b987f23fd5ea840df2b2be426315c-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [GPEX, A Framework For Interpreting Artificial Neural Networks](https://papers.nips.cc/paper_files/paper/2023/hash/ca8c6f28d8ba1e732e3f217ab05c4ec0-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers](https://papers.nips.cc/paper_files/paper/2023/hash/cdaac2a02c4fdcae77ba083b110efcc3-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned Samples in NLP](https://papers.nips.cc/paper_files/paper/2023/hash/d2b752ed4726286a4b488ae16e091d64-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [On the Identifiability and Interpretability of Gaussian Process Models](https://papers.nips.cc/paper_files/paper/2023/hash/dea2b4f9012686bcc1f59a62bcd28158-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [BasisFormer: Attention-based Time Series Forecasting with Learnable and Interpretable Basis](https://papers.nips.cc/paper_files/paper/2023/hash/e150e6d0a1e5214740c39c6e4503ba7a-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance](https://papers.nips.cc/paper_files/paper/2023/hash/e1f418450107c4a0ddc16d008d131573-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Evaluating Neuron Interpretation Methods of NLP Models](https://papers.nips.cc/paper_files/paper/2023/hash/eef6cb60fd59b32d35718e176b4b08d6-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [FIND: A Function Description Benchmark for Evaluating Interpretability Methods](https://papers.nips.cc/paper_files/paper/2023/hash/ef0164c1112f56246224af540857348f-Abstract-Datasets_and_Benchmarks.html) | NeurIPS | 2023 | N/A | `` | |
| [How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model](https://papers.nips.cc/paper_files/paper/2023/hash/efbba7719cc5172d175240f24be11280-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpretable Prototype-based Graph Information Bottleneck](https://papers.nips.cc/paper_files/paper/2023/hash/f224f056694bcfe465c5d84579785761-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [Interpretability at Scale: Identifying Causal Mechanisms in Alpaca](https://papers.nips.cc/paper_files/paper/2023/hash/f6a8b109d4d4fd64c75e94aaf85d9697-Abstract-Conference.html) | NeurIPS | 2023 | N/A | `` | |
| [M4: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models](https://papers.nips.cc/paper_files/paper/2023/hash/05957c194f4c77ac9d91e1374d2def6b-Abstract-Datasets_and_Benchmarks.html) | NeurIPS | 2023 | N/A | `` | |
| [InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning](https://aclanthology.org/2023.findings-emnlp.700/) | EMNLP | 2023 | N/A | `` | |
| [Towards Explainable and Accessible AI](https://aclanthology.org/2023.nlposs-1.28/) | EMNLP | 2023 | N/A | `` | |
| [KEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing](https://aclanthology.org/2023.emnlp-main.292/) | EMNLP | 2023 | N/A | `` | |
| [INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback](https://aclanthology.org/2023.emnlp-main.365/) | EMNLP | 2023 | N/A | `` | |
| [Goal-Driven Explainable Clustering via Language Descriptions](https://aclanthology.org/2023.emnlp-main.657/) | EMNLP | 2023 | N/A | `` | |
| [VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights](https://aclanthology.org/2023.emnlp-main.718/) | EMNLP | 2023 | N/A | `` | |
| [COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation](https://aclanthology.org/2023.emnlp-main.819/) | EMNLP | 2023 | N/A | `` | |
| [Hop, Union, Generate: Explainable Multi-hop Reasoning without Rationale Supervision](https://aclanthology.org/2023.emnlp-main.1001/) | EMNLP | 2023 | N/A | `` | |
| [GenEx: A Commonsense-aware Unified Generative Framework for Explainable Cyberbullying Detection](https://aclanthology.org/2023.emnlp-main.1035/) | EMNLP | 2023 | N/A | `` | |
| [DRGCoder: Explainable Clinical Coding for the Early Prediction of Diagnostic-Related Groups](https://aclanthology.org/2023.emnlp-demo.34/) | EMNLP | 2023 | N/A | `` | |
| [LLM4Vis: Explainable Visualization Recommendation using ChatGPT](https://aclanthology.org/2023.emnlp-industry.64/) | EMNLP | 2023 | N/A | `` | |
| [Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting](https://aclanthology.org/2023.emnlp-industry.69/) | EMNLP | 2023 | N/A | `` | |
| [HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning](https://aclanthology.org/2023.findings-emnlp.365/) | EMNLP | 2023 | N/A | `` | |
| [Distilling ChatGPT for Explainable Automated Student Answer Assessment](https://aclanthology.org/2023.findings-emnlp.399/) | EMNLP | 2023 | N/A | `` | |
| [Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models](https://aclanthology.org/2023.findings-emnlp.416/) | EMNLP | 2023 | N/A | `` | |
| [Leveraging Structured Information for Explainable Multi-hop Question Answering and Reasoning](https://aclanthology.org/2023.findings-emnlp.452/) | EMNLP | 2023 | N/A | `` | |
| [InstructSafety: A Unified Framework for Building Multidimensional and Explainable Safety Detector through Instruction Tuning](https://aclanthology.org/2023.findings-emnlp.700/) | EMNLP | 2023 | N/A | `` | |
| [Deep Integrated Explanations](https://dl.acm.org/doi/10.1145/3583780.3614836) | CIKM | 2023 | N/A | `` | |
| [KG4Ex: An Explainable Knowledge Graph-Based Approach for Exercise Recommendation](https://dl.acm.org/doi/10.1145/3583780.3614943) | CIKM | 2023 | N/A | `` | |
| [Interpretable Fake News Detection with Graph Evidence](https://dl.acm.org/doi/10.1145/3583780.3614936) | CIKM | 2023 | N/A | `` | |
| [PriSHAP: Prior-guided Shapley Value Explanations for Correlated Features](https://dl.acm.org/doi/10.1145/3583780.3615013) | CIKM | 2023 | N/A | `` | |
| [A Model-Agnostic Method to Interpret Link Prediction Evaluation of Knowledge Graph Embeddings](https://dl.acm.org/doi/10.1145/3583780.3614763) | CIKM | 2023 | N/A | `` | |
| [ACGAN-GNNExplainer: Auxiliary Conditional Generative Explainer for Graph Neural Networks](https://dl.acm.org/doi/10.1145/3583780.3614772) | CIKM | 2023 | N/A | `` | |
| [Concept Evolution in Deep Learning Training: A Unified Interpretation Framework and Discoveries](https://dl.acm.org/doi/10.1145/3583780.3614819) | CIKM | 2023 | N/A | `` | |
| [Explainable Spatio-Temporal Graph Neural Networks](https://dl.acm.org/doi/10.1145/3583780.3614871) | CIKM | 2023 | N/A | `` | |
| [Towards Deeper, Lighter and Interpretable Cross Network for CTR Prediction](https://dl.acm.org/doi/10.1145/3583780.3615089) | CIKM | 2023 | N/A | `` | |
| [Flexible and Robust Counterfactual Explanations with Minimal Satisfiable Perturbations](https://dl.acm.org/doi/10.1145/3583780.3614885) | CIKM | 2023 | N/A | `` | |
| [NOVO: Learnable and Interpretable Document Identifiers for Model-Based IR](https://dl.acm.org/doi/10.1145/3583780.3614993) | CIKM | 2023 | N/A | `` | |
| [Counterfactual Monotonic Knowledge Tracing for Assessing Students' Dynamic Mastery of Knowledge Concepts](https://dl.acm.org/doi/10.1145/3583780.3614827) | CIKM | 2023 | N/A | `` | |
| [Contrastive Counterfactual Learning for Causality-aware Interpretable Recommender Systems](https://dl.acm.org/doi/10.1145/3583780.3614823) | CIKM | 2023 | N/A | `` | |
| []() |  | 2023 | N/A | `` | |


### 2024
| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |
| [Interpretable Long-Form Legal Question Answering with Retrieval-Augmented Large Language Models](https://ojs.aaai.org/index.php/AAAI/article/view/30232) | AAAI | 2024 | N/A | `` | |
| [Evaluating Pre-trial Programs Using Interpretable Machine Learning Matching Algorithms for Causal Inference](https://ojs.aaai.org/index.php/AAAI/article/view/30239) | AAAI | 2024 | N/A | `` | |
| [On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods](https://ojs.aaai.org/index.php/AAAI/article/view/30082) | AAAI | 2024 | N/A | `` | |
| [A Framework for Data-Driven Explainability in Mathematical Optimization](https://ojs.aaai.org/index.php/AAAI/article/view/30081) | AAAI | 2024 | N/A | `` | |
| [Q-SENN: Quantized Self-Explaining Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/30145) | AAAI | 2024 | N/A | `` | |
| [LR-XFL: Logical Reasoning-Based Explainable Federated Learning](https://ojs.aaai.org/index.php/AAAI/article/view/30179) | AAAI | 2024 | N/A | `` | |
| [Trade-Offs in Fine-Tuned Diffusion Models between Accuracy and Interpretability](https://ojs.aaai.org/index.php/AAAI/article/view/30095) | AAAI | 2024 | N/A | `` | |
| [π-Light: Programmatic Interpretable Reinforcement Learning for Resource-Limited Traffic Signal Control](https://ojs.aaai.org/index.php/AAAI/article/view/30103) | AAAI | 2024 | N/A | `` | |
| [Interpretability Benchmark for Evaluating Spatial Misalignment of Prototypical Parts Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/30154) | AAAI | 2024 | N/A | `` | |
| [Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention](https://ojs.aaai.org/index.php/AAAI/article/view/30160) | AAAI | 2024 | N/A | `` | |
| [LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial Attack](https://ojs.aaai.org/index.php/AAAI/article/view/29950) | AAAI | 2024 | N/A | `` | |
| [Learning Robust Rationales for Model Explainability: A Guidance-Based Approach](https://ojs.aaai.org/index.php/AAAI/article/view/29783) | AAAI | 2024 | N/A | `` | |
| [Explaining Generalization Power of a DNN Using Interactive Concepts](https://ojs.aaai.org/index.php/AAAI/article/view/29655) | AAAI | 2024 | N/A | `` | |
| [Federated Causality Learning with Explainable Adaptive Optimizatio](https://ojs.aaai.org/index.php/AAAI/article/view/29566) | AAAI | 2024 | N/A | `` | |
| [Learning Performance Maximizing Ensembles with Explainability Guarantees](https://ojs.aaai.org/index.php/AAAI/article/view/29378) | AAAI | 2024 | N/A | `` | |
| [Towards Modeling Uncertainties of Self-Explaining Neural Networks via Conformal Prediction](https://ojs.aaai.org/index.php/AAAI/article/view/29382) | AAAI | 2024 | N/A | `` | |
| [Towards Learning and Explaining Indirect Causal Effects in Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/29399) | AAAI | 2024 | N/A | `` | |
| [GINN-LP: A Growing Interpretable Neural Network for Discovering Multivariate Laurent Polynomial Equations](https://ojs.aaai.org/index.php/AAAI/article/view/29396) | AAAI | 2024 | N/A | `` | |
| [Pantypes: Diverse Representatives for Self-Explainable Models](https://ojs.aaai.org/index.php/AAAI/article/view/29223) | AAAI | 2024 | N/A | `` | |
| [Factorized Explainer for Graph Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/29157) | AAAI | 2024 | N/A | `` | |
| [Self-Interpretable Graph Learning with Sufficient and Necessary Explanations](https://ojs.aaai.org/index.php/AAAI/article/view/29059) | AAAI | 2024 | N/A | `` | |
| [Learning from Ambiguous Demonstrations with Self-Explanation Guided Reinforcement Learning](https://ojs.aaai.org/index.php/AAAI/article/view/28907) | AAAI | 2024 | N/A | `` | |
| [A General Theoretical Framework for Learning Smallest Interpretable Models](https://ojs.aaai.org/index.php/AAAI/article/view/28937) | AAAI | 2024 | N/A | `` | |
| [Knowledge-Aware Explainable Reciprocal Recommendation](https://ojs.aaai.org/index.php/AAAI/article/view/28708) | AAAI | 2024 | N/A | `` | |
| [Fine-Tuning Large Language Model Based Explainable Recommendation with Explainable Quality Reward](https://ojs.aaai.org/index.php/AAAI/article/view/28777) | AAAI | 2024 | N/A | `` | |
| [Finding Interpretable Class-Specific Patterns through Efficient Neural Search](https://ojs.aaai.org/index.php/AAAI/article/view/28756) | AAAI | 2024 | N/A | `` | |
| [Enhance Sketch Recognition’s Explainability via Semantic Component-Level Parsing](https://ojs.aaai.org/index.php/AAAI/article/view/28607) | AAAI | 2024 | N/A | `` | |
| [B-spine: Learning B-spline Curve Representation for Robust and Interpretable Spinal Curvature Estimation](https://ojs.aaai.org/index.php/AAAI/article/view/28346) | AAAI | 2024 | N/A | `` | |
| [A Convolutional Neural Network Interpretable Framework for Human Ventral Visual Pathway Representation](https://ojs.aaai.org/index.php/AAAI/article/view/28461) | AAAI | 2024 | N/A | `` | |
| [NeSyFOLD: A Framework for Interpretable Image Classification](https://ojs.aaai.org/index.php/AAAI/article/view/28235) | AAAI | 2024 | N/A | `` | |
| [Knowledge-Aware Neuron Interpretation for Scene Classification](https://ojs.aaai.org/index.php/AAAI/article/view/27965) | AAAI | 2024 | N/A | `` | |
| [MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level Image-Concept Alignment](https://ojs.aaai.org/index.php/AAAI/article/view/27842) | AAAI | 2024 | N/A | `` | |
| [Interpretable3D: An Ad-Hoc Interpretable Classifier for 3D Point Clouds](https://ojs.aaai.org/index.php/AAAI/article/view/27944) | AAAI | 2024 | N/A | `` | |
| [Learning Audio Concepts from Counterfactual Natural Language](https://ieeexplore.ieee.org/document/10446736) | ICASSP | 2024 | N/A | `` | |
| [Unifying Interpretability and Explainability for Alzheimer's Disease Progression Prediction](https://arxiv.org/abs/2406.07777) | Arxiv | 2024 | [Code](https://github.com/rfali/xrlad) | `` | |
| []() |  | 2024 | N/A | `` | |
| [A Brain-Inspired Way of Reducing the Network Complexity via Concept-Regularized Coding for Emotion Recognition](https://ojs.aaai.org/index.php/AAAI/article/view/27811) | AAAI | 2024 | N/A | `` | |
| [Visual Chain-of-Thought Prompting for Knowledge-Based Visual Reasoning](https://ojs.aaai.org/index.php/AAAI/article/view/27888) | AAAI | 2024 | N/A | `` | |
| [Beyond Prototypes: Semantic Anchor Regularization for Better Representation Learning](https://ojs.aaai.org/index.php/AAAI/article/view/27958) | AAAI | 2024 | N/A | `` | |
| [PICNN: A Pathway towards Interpretable Convolutional Neural Networks](https://ojs.aaai.org/index.php/AAAI/article/view/27971) | AAAI | 2024 | N/A | `` | |
| [MagiCapture: High-Resolution Multi-Concept Portrait Customization](https://ojs.aaai.org/index.php/AAAI/article/view/28020) | AAAI | 2024 | N/A | `` | |
| [AMD: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion](https://ojs.aaai.org/index.php/AAAI/article/view/28042) | AAAI | 2024 | N/A | `` | |
| [Towards More Faithful Natural Language Explanation Using Multi-Level Contrastive Learning in VQA](https://ojs.aaai.org/index.php/AAAI/article/view/28065) | AAAI | 2024 | N/A | `` | |
| [ViTree: Single-Path Neural Tree for Step-Wise Interpretable Fine-Grained Visual Categorization](https://ojs.aaai.org/index.php/AAAI/article/view/28067) | AAAI | 2024 | N/A | `` | |
| [Text-to-Image Generation for Abstract Concepts](https://ojs.aaai.org/index.php/AAAI/article/view/28122) | AAAI | 2024 | N/A | `` | |
| [Boosting Multiple Instance Learning Models for Whole Slide Image Classification: A Model-Agnostic Framework Based on Counterfactual Inference](https://ojs.aaai.org/index.php/AAAI/article/view/28135) | AAAI | 2024 | N/A | `` | |
| [Set Prediction Guided by Semantic Concepts for Diverse Video Captioning](https://ojs.aaai.org/index.php/AAAI/article/view/28183) | AAAI | 2024 | N/A | `` | |
| [Understanding the Role of the Projector in Knowledge Distillation](https://ojs.aaai.org/index.php/AAAI/article/view/28219) | AAAI | 2024 | N/A | `` | |
| [Concept-Guided Prompt Learning for Generalization in Vision-Language Models](https://ojs.aaai.org/index.php/AAAI/article/view/28568) | AAAI | 2024 | N/A | `` | |
| [Automatic Core-Guided Reformulation via Constraint Explanation and Condition Learning](https://ojs.aaai.org/index.php/AAAI/article/view/28645) | AAAI | 2024 | N/A | `` | |
| [Learning to Pivot as a Smart Expert](https://ojs.aaai.org/index.php/AAAI/article/view/28646) | AAAI | 2024 | N/A | `` | |
| [Explainable Origin-Destination Crowd Flow Interpolation via Variational Multi-Modal Recurrent Graph Auto-Encoder](https://ojs.aaai.org/index.php/AAAI/article/view/28796) | AAAI | 2024 | N/A | `` | |
| [Explaining Reinforcement Learning Agents through Counterfactual Action Outcomes](https://ojs.aaai.org/index.php/AAAI/article/view/28863) | AAAI | 2024 | N/A | `` | |
| [Understanding Distributed Representations of Concepts in Deep Neural Networks without Supervision](https://ojs.aaai.org/index.php/AAAI/article/view/28999) | AAAI | 2024 | N/A | `` | |
| [Unsupervised Object Interaction Learning with Counterfactual Dynamics Models](https://ojs.aaai.org/index.php/AAAI/article/view/29039) | AAAI | 2024 | N/A | `` | |
| [NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models](https://openaccess.thecvf.com/content/CVPR2024/html/Dalva_NoiseCLR_A_Contrastive_Learning_Approach_for_Unsupervised_Discovery_of_Interpretable_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations](https://openaccess.thecvf.com/content/CVPR2024/html/Chakraborty_ExMap_Leveraging_Explainability_Heatmaps_for_Unsupervised_Group_Robustness_to_Spurious_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Interpretable Measures of Conceptual Similarity by Complexity-Constrained Descriptive Auto-Encoding](https://openaccess.thecvf.com/content/CVPR2024/html/Achille_Interpretable_Measures_of_Conceptual_Similarity_by_Complexity-Constrained_Descriptive_Auto-Encoding_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions](https://openaccess.thecvf.com/content/CVPR2024/html/Padmanabhan_Explaining_the_Implicit_Neural_Canvas_Connecting_Pixels_to_Neurons_by_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models](https://openaccess.thecvf.com/content/CVPR2024/html/Kowal_Visual_Concept_Connectome_VCC_Open_World_Concept_Discovery_and_their_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Link-Context Learning for Multimodal LLMs](https://openaccess.thecvf.com/content/CVPR2024/html/Tai_Link-Context_Learning_for_Multimodal_LLMs_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Explaining CLIP's Performance Disparities on Data from Blind/Low Vision Users](https://openaccess.thecvf.com/content/CVPR2024/html/Massiceti_Explaining_CLIPs_Performance_Disparities_on_Data_from_BlindLow_Vision_Users_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Learning Structure-from-Motion with Graph Attention Networks](https://openaccess.thecvf.com/content/CVPR2024/html/Brynte_Learning_Structure-from-Motion_with_Graph_Attention_Networks_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding](https://openaccess.thecvf.com/content/CVPR2024/html/Wang_GroupContrast_Semantic-aware_Self-supervised_Representation_Learning_for_3D_Understanding_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Building Optimal Neural Architectures using Interpretable Knowledge](https://openaccess.thecvf.com/content/CVPR2024/html/Mills_Building_Optimal_Neural_Architectures_using_Interpretable_Knowledge_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Understanding Video Transformers via Universal Concept Discovery](https://openaccess.thecvf.com/content/CVPR2024/html/Kowal_Understanding_Video_Transformers_via_Universal_Concept_Discovery_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [A Unified and Interpretable Emotion Representation and Expression Generation](https://openaccess.thecvf.com/content/CVPR2024/html/Paskaleva_A_Unified_and_Interpretable_Emotion_Representation_and_Expression_Generation_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
| [Data Poisoning based Backdoor Attacks to Contrastive Learning](https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_Data_Poisoning_based_Backdoor_Attacks_to_Contrastive_Learning_CVPR_2024_paper.html) | CVPR | 2024 | N/A | `` | |
