# Recent Publications in Explainable AI and their summaries
A repository for summaries of recent explainable AI/Interpretable ML approaches


| Title                                                                                                                                                                                                                    |            Venue          |         Year         |        Code        |                  Keywords                  |              Summary              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  | :------------------------ | :------------------- | :----------------- | :----------------------------------------- | :-------------------------------- |

| [Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission](https://people.dbmi.columbia.edu/noemie/papers/15kdd.pdf) | KDD | 2015 | N/A | `` | [Link]() |
| [Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model](https://arxiv.org/abs/1511.01644) | arXiv | 2015 | N/A | `` | [Link]() |
| [Interpretable Decision Sets: A Joint Framework for Description and Prediction](https://www-cs-faculty.stanford.edu/people/jure/pubs/interpretable-kdd16.pdf) | KDD | 2016 | N/A | `` | [Link]() |
| ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) | KDD | 2016 | N/A | `` | [Link]() |
| [Towards A Rigorous Science of Interpretable Machine Learning](https://arxiv.org/abs/1702.08608) | arXiv | 2017 | N/A | `Review Paper` | [Link]() |
| [Transparency: Motivations and Challenges](https://arxiv.org/abs/1708.01870) | arXiv | 2017 | N/A | `Review Paper` | [Link]() |
| [A Unified Approach to Interpreting Model Predictions](https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf) | NeurIPS | 2017 | N/A | `` | [Link]() |
| [SmoothGrad: removing noise by adding noise](https://arxiv.org/abs/1706.03825) | ICML (Workshop) | 2017 | [Github](https://github.com/pair-code/saliency) | `` | [Link]() |
| [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) | ICML | 2017 | N/A | `` | [Link]() |
| [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) | ICML | 2017 | N/A | `` | [Link]() |
| [Understanding Black-box Predictions via Influence Functions](https://arxiv.org/abs/1703.04730) | ICML | 2017 | N/A | `` | [Link]() |
| [Network Dissection: Quantifying Interpretability of Deep Visual Representations](http://netdissect.csail.mit.edu/final-network-dissection.pdf) | CVPR | 2017 | N/A | `` | [Link]() |
| [Explainable Prediction of Medical Codes from Clinical Text](https://aclanthology.org/N18-1100.pdf) | ACL | 2018 | N/A | `` | [Link]() |
| [Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)](https://arxiv.org/abs/1711.11279) | ICML | 2018 | N/A | `` | [Link]() |
| [Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR](https://arxiv.org/abs/1711.00399) | HJTL | 2018 | N/A | `` | [Link]() |
| [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292) | NeruIPS | 2018 | N/A | `` | [Link]() |
| [Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions](https://arxiv.org/abs/1710.04806) | AAAI | 2018 | N/A | `` | [Link]() |
| [The Mythos of Model Interpretability](https://dl.acm.org/doi/10.1145/3236386.3241340) | arXiv | 2018 | N/A | `Review Paper` | [Link]() |
| [Human Evaluation of Models Built for Interpretability](https://ojs.aaai.org/index.php/HCOMP/article/view/5280/5132) | AAAI | 2019 | N/A | `Human in the loop` | [Link]() |
| [Data Shapley: Equitable Valuation of Data for Machine Learning](https://arxiv.org/abs/1904.02868) | ICML | 2019 | N/A | `` | [Link]() |
| [Attention is not Explanation](https://arxiv.org/abs/1902.10186) | ACL | 2019 | N/A | `` | [Link]() |
| [Actionable Recourse in Linear Classification](https://arxiv.org/abs/1809.06514) | FAccT | 2019 | N/A | `` | [Link]() |
| [Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead](https://arxiv.org/abs/1811.10154) | Nature | 2019 | N/A | `` | [Link]() |
| [Explanations can be manipulated and geometry is to blame](https://arxiv.org/abs/1906.07983) | NeurIPS | 2019 | N/A | `` | [Link]() |
| [Learning Optimized Risk Scores](https://arxiv.org/pdf/1610.00168.pdf) | JMLR | 2019 | N/A | `` | [Link]() |
| [Explain Yourself! Leveraging Language Models for Commonsense Reasoning](https://arxiv.org/abs/1906.02361) | ACL | 2019 | N/A | `` | [Link]() |
| [Interpreting the Latent Space of GANs for Semantic Face Editing](https://arxiv.org/abs/1907.10786) | CVPR | 2020 | N/A | `` | [Link]() |
| [GANSpace: Discovering Interpretable GAN Controls](https://arxiv.org/abs/2004.02546) | NeurIPS | 2020 | N/A | `` | [Link]() |
| [Explainability for fair machine learning](https://arxiv.org/abs/2010.07389) | arXiv | 2020 | N/A | `` | [Link]() |
| [An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/) | Distill | 2020 | N/A | `Tutorial` | [Link]() |
| [Beyond Individualized Recourse: Interpretable and Interactive Summaries of Actionable Recourses](https://arxiv.org/abs/2009.07165) | NeurIPS | 2020 | N/A | `` | [Link]() |
| [Learning Model-Agnostic Counterfactual Explanations for Tabular Data](https://arxiv.org/abs/1910.09398) | WWW | 2020 | N/A | `` | [Link]() |
| [Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods](https://arxiv.org/abs/1911.02508) | AIES (AAAI) | 2020 | N/A | `` | [Link]() |
| [Interpreting Interpretability: Understanding Data Scientists’ Use of Interpretability Tools for Machine Learning](http://www-personal.umich.edu/~harmank/Papers/CHI2020_Interpretability.pdf) | CHI | 2020 | N/A | `Review Paper` | [Link]() |
| [Human Factors in Model Interpretability: Industry Practices, Challenges, and Needs](https://dl.acm.org/doi/10.1145/3392878) | arXiv | 2020 | N/A | `Review Paper` | [Link]() |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR (Poster) | 2021 | N/A | `` | [Link]() |
| [A Learning Theoretic Perspective on Local Explainability](https://arxiv.org/abs/2011.01205) | ICLR | 2021 | N/A | `` | [Link]() |
| [Do Input Gradients Highlight Discriminative Features?](https://arxiv.org/abs/2102.12781) | NeurIPS | 2021 | N/A | `` | [Link]() |
| [Explaining by Removing: A Unified Framework for Model Explanation](https://www.jmlr.org/papers/volume22/20-1316/20-1316.pdf) | JMLR | 2021 | N/A | `` | [Link]() |
| [Explainable Active Learning (XAL): An Empirical Study of How Local Explanations Impact Annotator Experience](https://arxiv.org/abs/2001.09219) | PACMHCI | 2021 | N/A | `` | [Link]() |
| [Towards Robust and Reliable Algorithmic Recourse](https://arxiv.org/abs/2102.13620) | NeurIPS | 2021 | N/A | `` | [Link]() |
| [Algorithmic Recourse: from Counterfactual Explanations to Interventions](https://arxiv.org/abs/2002.06278) | FAccT | 2021 | N/A | `` | [Link]() |
| [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) | CHI | 2021 | N/A | `` | [Link]() |
| [TalkToModel: Explaining Machine Learning Models with Interactive Natural Language Conversations](https://arxiv.org/abs/2207.04154) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases](https://transformer-circuits.pub/2022/mech-interp-essay/index.html) | Transformer Circuit Thread | 2022 | N/A | `Tutorial` | [Link]() |
| [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903) | NeurIPS | 2022 | N/A | `Google Brain` | [Link]() |
| [Can language models learn from explanations in context?](https://arxiv.org/abs/2204.02329) | EMNLP | 2022 | N/A | `DeepMind` | [Link]() |
| [Interpreting Language Models with Contrastive Explanations](https://arxiv.org/abs/2202.10419) | EMNLP | 2022 | N/A | `` | [Link]() |
| [Acquisition of Chess Knowledge in AlphaZero](https://arxiv.org/pdf/2111.09259.pdf) | PNAS | 2022 | N/A | `DeepMind` `Google Brain` | [Link]() |
| [What the DAAM: Interpreting Stable Diffusion Using Cross Attention](https://arxiv.org/abs/2210.04885) | arXiv | 2022 | [Github](https://github.com/castorini/daam) | `` | [Link]() |
| [Exploring Counterfactual Explanations Through the Lens of Adversarial Examples: A Theoretical and Empirical Analysis](https://arxiv.org/abs/2106.09992) | AISTATS | 2022 | N/A | `` | [Link]() |
| [Use-Case-Grounded Simulations for Explanation Evaluation](https://arxiv.org/abs/2206.02256) | NIPS | 2022 | N/A | `` | [Link]() |
| [OpenXAI: Towards a Transparent Evaluation of Model Explanations](https://arxiv.org/abs/2206.11104) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective](https://arxiv.org/abs/2202.01602) | arXiv | 2022 | N/A | `` | [Link]() |
| [What Makes a Good Explanation?: A Harmonized View of Properties of Explanations](https://arxiv.org/abs/2211.05667) | arXiv | 2022 | N/A | `` | [Link]() |
| [Which Explanation Should I Choose? A Function Approximation Perspective to Characterizing Post Hoc Explanations](https://arxiv.org/abs/2206.01254) | NeurIPS | 2022 | N/A | `` | [Link]() |
| [NoiseGrad — Enhancing Explanations by Introducing Stochasticity to Model Weights](https://cdn.aaai.org/ojs/20561/20561-13-24574-1-2-20220628.pdf) | AAAI | 2022 | [Github](https://github.com/understandable-machine-intelligence-lab/NoiseGrad) | `` | [Link]() |
| [Fairness via Explanation Quality: Evaluating Disparities in the Quality of Post hoc Explanations](https://arxiv.org/abs/2205.07277) | AIES (AAAI) | 2022 | N/A | `` | [Link]() |
| [DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Models](https://arxiv.org/abs/2202.04053) | arXiv | 2022 | [Github](https://github.com/j-min/DallEval) | `` | [Link]() |
| [On the Privacy Risks of Algorithmic Recourse](https://arxiv.org/abs/2211.05427) | AISTATS | 2023 | N/A | `` | [Link]() |
| [Towards Bridging the Gaps between the Right to Explanation and the Right to be Forgotten](https://arxiv.org/abs/2302.04288) | ICML | 2023 | N/A | `` | [Link]() |
| [Tracr: Compiled Transformers as a Laboratory for Interpretability](https://arxiv.org/abs/2301.05062) | arXiv | 2023 | [Github](https://github.com/deepmind/tracr) | `DeepMind` | [Link]() |
| [Probabilistically Robust Recourse: Navigating the Trade-offs between Costs and Robustness in Algorithmic Recourse](https://arxiv.org/abs/2203.06768) | ICLR | 2023 | N/A | `` | [Link]() |
| []() |  |  | N/A | `` | [Link]() |
| []() |  |  | N/A | `` | [Link]() |
| []() |  |  | N/A | `` | [Link]() |
